# analyzer.py
# æ ¸å¿ƒåˆ†æé‚è¼¯æ¨¡çµ„

import os, re, json, asyncio, logging
import pandas as pd 
import numpy as np
import google.generativeai as genai
from sklearn.metrics.pairwise import cosine_similarity
import yaml # æ–°å¢åŒ¯å…¥ yaml

# --- Gemini API è¨­å®š ---
def configure_gemini(api_key):
    """è¨­å®š Gemini API é‡‘é‘°ä¸¦åˆå§‹åŒ–æ¨¡å‹"""
    global gmodel
    try:
        genai.configure(api_key=api_key)
        gmodel = genai.GenerativeModel("gemini-1.5-pro-latest")
        return True
    except Exception as e:
        logging.error(f"è¨­å®š Gemini API é‡‘é‘°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

gmodel = None

# --- ä½¿ç”¨è€…å¯ä¿®æ”¹åƒæ•¸ ---
TARGET_SCORE_Q = [1, 2, 3, 4, 11]
BATCH_SIZE = 6
DO_SIMILARITY_CHECK = True

# --- è©•åˆ†æ¨™æº– (Rubrics) ---
# ç‚ºç¯€çœç©ºé–“ï¼Œæ­¤è™•çœç•¥è©³ç´°å…§å®¹ï¼Œè«‹å°‡æ‚¨å…ˆå‰ç‰ˆæœ¬çš„å®Œæ•´ RUBRICS å­—å…¸è²¼åˆ°æ­¤è™•
RUBRICS = {
    1: '''Rubric (10 pts total)\nSection A â€“ CZ vs FZ singleâ€‘crystal Si comparison (0â€‘4 pts)...''',
    2: '''Rubric (10 pts total) â€” Student **answers any TWO** subâ€‘problems...''',
    3: '''Rubric (10 pts total) â€” Student **answers any TWO** subâ€‘problems...''',
    4: '''Rubric (10 pts total) â€” Student **answers any TWO** subâ€‘problems...''',
    11: '''Rubric (10 pts total)\nPartâ€¯1  Quantum confinement in 0â€‘D nanomaterials (0â€‘5)...''',
}

# --- Prompt æ¨¡æ¿ ---
PROMPT_TEMPLATE = """You are a meticulous graduate-level TA.
First, judge whether the following answer exhibits obvious LLM writing style (including but not limited to overly smooth transitions, template phrases like 'moreover', almost no typos).
Return an integer 'ai_risk' from 0â€‘100 (higher = more AI-like).
{grade_block}
Respond ONLY with a valid JSON object like {{"ai_risk": ..., "score": ...}}.

Question:
{question}

Answer:
{answer}
"""

def log_to_frontend(message, callback):
    """å®‰å…¨åœ°å‘¼å«å‰ç«¯æ—¥èªŒå›å‘¼å‡½å¼"""
    if callback:
        callback(message)
    else:
        print(message)

def load_exam(csv_path, log_callback):
    """å¾ CSV æª”æ¡ˆè¼‰å…¥è€ƒå·ç­”æ¡ˆ"""
    try:
        df = pd.read_csv(
            csv_path,
            encoding='utf-8',
            dtype=str,
            quotechar='"',
            escapechar='\\'
        )
        # å»é™¤æ¬„ä½åç¨±å‰å¾Œç©ºç™½ï¼Œä¸¦å°‡æ›è¡Œè½‰ç‚ºç©ºæ ¼
        df.columns = (
            df.columns
            .str.strip()
            .str.replace('\n', ' ', regex=False)
        )
    except FileNotFoundError:
        log_to_frontend(f"âŒ æ‰¾ä¸åˆ°æŒ‡å®šçš„æª”æ¡ˆ: {csv_path}", log_callback)
        return None, None
    
    # æ‰¾åˆ°çµæ§‹æ¨™è¨˜æ¬„ä½çš„ç´¢å¼•
    try:
        attempt_idx = df.columns.get_loc('attempt')
        n_correct_idx = df.columns.get_loc('n correct')
    except KeyError as e:
        log_to_frontend(f"âŒ æ‰¾ä¸åˆ°å¿…è¦çš„æ¬„ä½: {e}ï¼Œè«‹æª¢æŸ¥CSVæ ¼å¼", log_callback)
        return None, None
    
    # è§£æattemptå’Œn correctä¹‹é–“çš„é¡Œç›®/åˆ†æ•¸é…å°
    q_map = {}
    q_counter = 1
    
    for i in range(attempt_idx + 1, n_correct_idx, 2):
        if i + 1 < n_correct_idx:  # ç¢ºä¿æœ‰é…å°çš„åˆ†æ•¸æ¬„ä½
            question_col = df.columns[i]
            score_col = df.columns[i + 1]
            
            # æª¢æŸ¥åˆ†æ•¸æ¬„ä½æ˜¯å¦ç‚ºæ•¸å€¼å‹æ…‹æˆ–åŒ…å«åˆ†æ•¸è³‡è¨Š
            if (pd.to_numeric(df[score_col], errors='coerce').notna().any() or 
                any(str(val).replace('.', '').isdigit() for val in df[score_col].dropna())):
                q_map[q_counter] = question_col
                q_counter += 1
                log_to_frontend(f"  æ‰¾åˆ°é¡Œç›® {q_counter-1}: {question_col[:50]}...", log_callback)
    
    log_to_frontend(f"âœ… æˆåŠŸè¼‰å…¥ {os.path.basename(csv_path)}ï¼Œæ‰¾åˆ° {len(df)} ä½å­¸ç”Ÿèˆ‡ {len(q_map)} é¡Œå•ç­”ã€‚", log_callback)
    return df, q_map

def calculate_similarity_flags(texts: list[str], hi=0.85, mid=0.70) -> list[int]:
    """è¨ˆç®—ç­”æ¡ˆé–“çš„èªæ„ç›¸ä¼¼åº¦"""
    if not DO_SIMILARITY_CHECK or len(texts) < 2:
        return [0] * len(texts)
    try:
        safe_texts = [t if t and t.strip() else " " for t in texts]
        result = genai.embed_content(model="models/text-embedding-004", content=safe_texts, task_type="RETRIEVAL_DOCUMENT")
        embs = result['embedding']
        sims = cosine_similarity(embs)
        np.fill_diagonal(sims, 0)
        max_sims = np.max(sims, axis=1)
        return [2 if s >= hi else 1 if s >= mid else 0 for s in max_sims]
    except Exception as e:
        logging.error(f"åŸ·è¡Œç›¸ä¼¼åº¦è¨ˆç®—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return [-1] * len(texts)

async def gemini_eval(question: str, rubric: str, answer: str, need_score: bool) -> tuple:
    """éåŒæ­¥å‘¼å« Gemini API é€²è¡Œ AI é¢¨æ ¼åˆ†æèˆ‡è©•åˆ†"""
    if not answer or not answer.strip():
        return (0, 0 if need_score else None)
    grade_block = f"Then grade the answer..." if need_score else "Grading is not required..."
    prompt = PROMPT_TEMPLATE.format(question=question, answer=answer, grade_block=grade_block)
    try:
        resp = await gmodel.generate_content_async(
            prompt,
            generation_config=genai.types.GenerationConfig(temperature=0.0, response_mime_type="application/json"),
            request_options={"timeout": 120}
        )
        if not resp.text: return (None, None)
        js = json.loads(resp.text)
        return (js.get("ai_risk"), js.get("score"))
    except Exception as e:
        logging.error(f"Gemini API å‘¼å«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return (None, None)

async def process_question(df, qid, col, log_callback):
    """è™•ç†å–®ä¸€å•é¡Œçš„æ‰€æœ‰å­¸ç”Ÿç­”æ¡ˆ"""
    log_to_frontend(f"â¡ï¸ é–‹å§‹è™•ç† Q{qid}...", log_callback)
    q_text = col.split(":", 1)[1].strip()
    need_score = qid in TARGET_SCORE_Q
    rubric = RUBRICS.get(qid, "")
    
    sub = df[["name", col]].rename(columns={col: "answer"}).fillna("")
    sub[f"Q{qid}_sim_flag"] = calculate_similarity_flags(sub["answer"].tolist())
    
    out_ai, out_sc = [], []
    chunks = [sub.iloc[i:i + BATCH_SIZE] for i in range(0, len(sub), BATCH_SIZE)]
    
    for i, chunk in enumerate(chunks):
        log_to_frontend(f"    Q{qid}: è™•ç†æ‰¹æ¬¡ {i+1}/{len(chunks)}", log_callback)
        tasks = [gemini_eval(q_text, rubric, a, need_score) for a in chunk["answer"]]
        results = await asyncio.gather(*tasks)
        out_ai.extend([r[0] for r in results])
        out_sc.extend([r[1] for r in results])
        
    sub[f"Q{qid}_ai_risk"] = out_ai
    if need_score:
        sub[f"Q{qid}_score"] = out_sc
        
    return sub.drop(columns=["answer"])

async def run_analysis(api_key: str, csv_path: str, out_base_path: str, log_callback):
    """åŸ·è¡Œå®Œæ•´åˆ†ææµç¨‹çš„ä¸»å‡½å¼"""
    if not configure_gemini(api_key):
        log_to_frontend("âŒ API é‡‘é‘°è¨­å®šå¤±æ•—ï¼Œè«‹æª¢æŸ¥é‡‘é‘°æ˜¯å¦æ­£ç¢ºã€‚", log_callback)
        return

    df, qmap = load_exam(csv_path, log_callback)
    if df is None: return

    merged_df = df[["name"]].copy()
    
    for qid, col in qmap.items():
        res_df = await process_question(df, qid, col, log_callback)
        merged_df = merged_df.merge(res_df, on="name", how="left")
        
    # å®šç¾©å„ç¨®æ ¼å¼çš„è¼¸å‡ºè·¯å¾‘
    xlsx_path = f"{out_base_path}.xlsx"
    csv_path_out = f"{out_base_path}.csv"
    yaml_path = f"{out_base_path}.yaml"
    html_path = f"{out_base_path}.html"

    try:
        # å„²å­˜ç‚º Excel
        merged_df.to_excel(xlsx_path, index=False, engine='openpyxl')
        log_to_frontend(f"ğŸ‰ Excel å ±å‘Šå·²å„²å­˜è‡³ï¼š\n{xlsx_path}", log_callback)

        # å„²å­˜ç‚º CSV
        merged_df.to_csv(csv_path_out, index=False)
        log_to_frontend(f"ğŸ‰ CSV å ±å‘Šå·²å„²å­˜è‡³ï¼š\n{csv_path_out}", log_callback)

        # å„²å­˜ç‚º YAML
        # å°‡ DataFrame è½‰æ›ç‚ºå­—å…¸åˆ—è¡¨ä»¥ä¾¿ YAML åºåˆ—åŒ–
        yaml_data = merged_df.to_dict(orient='records')
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(yaml_data, f, allow_unicode=True, sort_keys=False)
        log_to_frontend(f"ğŸ‰ YAML å ±å‘Šå·²å„²å­˜è‡³ï¼š\n{yaml_path}", log_callback)

        # å„²å­˜ç‚º HTML
        # ä½¿ç”¨ DataFrame.to_html() æ–¹æ³•ï¼Œå¯ä»¥åŠ å…¥ä¸€äº›æ¨£å¼
        html_content = merged_df.to_html(index=False, escape=False, classes='table table-striped')
        # å¯ä»¥é¸æ“‡æ€§åœ°åŠ å…¥ä¸€äº›åŸºæœ¬çš„ HTML çµæ§‹å’Œ CSS æ¨£å¼
        html_output = f"""
        <html>
        <head>
            <title>åˆ†æå ±å‘Š</title>
            <style>
                body {{ font-family: sans-serif; margin: 20px; }}
                .table {{ width: 100%; border-collapse: collapse; }}
                .table th, .table td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                .table th {{ background-color: #f2f2f2; }}
                .table-striped tbody tr:nth-of-type(odd) {{ background-color: #f9f9f9; }}
            </style>
        </head>
        <body>
            <h1>åˆ†æå ±å‘Š</h1>
            {html_content}
        </body>
        </html>
        """
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_output)
        log_to_frontend(f"ğŸ‰ HTML å ±å‘Šå·²å„²å­˜è‡³ï¼š\n{html_path}", log_callback)

    except Exception as e:
        log_to_frontend(f"âŒ å„²å­˜å ±å‘Šå¤±æ•—: {e}", log_callback)