# analyzer.py
# æ ¸å¿ƒåˆ†æé‚è¼¯æ¨¡çµ„

import os, re, json, asyncio, logging
import pandas as pd 
import numpy as np
import google.generativeai as genai
from sklearn.metrics.pairwise import cosine_similarity

# --- Gemini API è¨­å®š ---
def configure_gemini(api_key):
    """è¨­å®š Gemini API é‡‘é‘°ä¸¦åˆå§‹åŒ–æ¨¡å‹"""
    global gmodel
    try:
        genai.configure(api_key=api_key)
        gmodel = genai.GenerativeModel("gemini-1.5-pro-latest")
        return True
    except Exception as e:
        logging.error(f"è¨­å®š Gemini API é‡‘é‘°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

gmodel = None

# --- ä½¿ç”¨è€…å¯ä¿®æ”¹åƒæ•¸ ---
TARGET_SCORE_Q = [1, 2, 3, 4, 11]
BATCH_SIZE = 6
DO_SIMILARITY_CHECK = True

# --- è©•åˆ†æ¨™æº– (Rubrics) ---
# ç‚ºç¯€çœç©ºé–“ï¼Œæ­¤è™•çœç•¥è©³ç´°å…§å®¹ï¼Œè«‹å°‡æ‚¨å…ˆå‰ç‰ˆæœ¬çš„å®Œæ•´ RUBRICS å­—å…¸è²¼åˆ°æ­¤è™•
RUBRICS = {
    1: '''Rubric (10 pts total)\nSection A â€“ CZ vs FZ singleâ€‘crystal Si comparison (0â€‘4 pts)...''',
    2: '''Rubric (10 pts total) â€” Student **answers any TWO** subâ€‘problems...''',
    3: '''Rubric (10 pts total) â€” Student **answers any TWO** subâ€‘problems...''',
    4: '''Rubric (10 pts total) â€” Student **answers any TWO** subâ€‘problems...''',
    11: '''Rubric (10 pts total)\nPartâ€¯1  Quantum confinement in 0â€‘D nanomaterials (0â€‘5)...''',
}

# --- Prompt æ¨¡æ¿ ---
PROMPT_TEMPLATE = """You are a meticulous graduate-level TA.
First, judge whether the following answer exhibits obvious LLM writing style (including but not limited to overly smooth transitions, template phrases like 'moreover', almost no typos).
Return an integer 'ai_risk' from 0â€‘100 (higher = more AI-like).
{grade_block}
Respond ONLY with a valid JSON object like {{"ai_risk": ..., "score": ...}}.

Question:
{question}

Answer:
{answer}
"""

def log_to_frontend(message, callback):
    """å®‰å…¨åœ°å‘¼å«å‰ç«¯æ—¥èªŒå›å‘¼å‡½å¼"""
    if callback:
        callback(message)
    else:
        print(message)

def load_exam(csv_path, log_callback):
    """å¾ CSV æª”æ¡ˆè¼‰å…¥è€ƒå·ç­”æ¡ˆ"""
    try:
        df = pd.read_csv(csv_path)
    except FileNotFoundError:
        log_to_frontend(f"âŒ æ‰¾ä¸åˆ°æŒ‡å®šçš„æª”æ¡ˆ: {csv_path}", log_callback)
        return None, None
    ans_cols = [c for c in df.columns if re.match(r'^(352|362)\d+:', str(c))]
    q_map = {i+1: col for i, col in enumerate(ans_cols)}
    log_to_frontend(f"âœ… æˆåŠŸè¼‰å…¥ {os.path.basename(csv_path)}ï¼Œæ‰¾åˆ° {len(df)} ä½å­¸ç”Ÿèˆ‡ {len(q_map)} é¡Œå•ç­”ã€‚", log_callback)
    return df, q_map

def calculate_similarity_flags(texts: list[str], hi=0.85, mid=0.70) -> list[int]:
    """è¨ˆç®—ç­”æ¡ˆé–“çš„èªæ„ç›¸ä¼¼åº¦"""
    if not DO_SIMILARITY_CHECK or len(texts) < 2:
        return [0] * len(texts)
    try:
        safe_texts = [t if t and t.strip() else " " for t in texts]
        result = genai.embed_content(model="models/text-embedding-004", content=safe_texts, task_type="RETRIEVAL_DOCUMENT")
        embs = result['embedding']
        sims = cosine_similarity(embs)
        np.fill_diagonal(sims, 0)
        max_sims = np.max(sims, axis=1)
        return [2 if s >= hi else 1 if s >= mid else 0 for s in max_sims]
    except Exception as e:
        logging.error(f"åŸ·è¡Œç›¸ä¼¼åº¦è¨ˆç®—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return [-1] * len(texts)

async def gemini_eval(question: str, rubric: str, answer: str, need_score: bool) -> tuple:
    """éåŒæ­¥å‘¼å« Gemini API é€²è¡Œ AI é¢¨æ ¼åˆ†æèˆ‡è©•åˆ†"""
    if not answer or not answer.strip():
        return (0, 0 if need_score else None)
    grade_block = f"Then grade the answer..." if need_score else "Grading is not required..."
    prompt = PROMPT_TEMPLATE.format(question=question, answer=answer, grade_block=grade_block)
    try:
        resp = await gmodel.generate_content_async(
            prompt,
            generation_config=genai.types.GenerationConfig(temperature=0.0, response_mime_type="application/json"),
            request_options={"timeout": 120}
        )
        if not resp.text: return (None, None)
        js = json.loads(resp.text)
        return (js.get("ai_risk"), js.get("score"))
    except Exception as e:
        logging.error(f"Gemini API å‘¼å«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return (None, None)

async def process_question(df, qid, col, log_callback):
    """è™•ç†å–®ä¸€å•é¡Œçš„æ‰€æœ‰å­¸ç”Ÿç­”æ¡ˆ"""
    log_to_frontend(f"â¡ï¸ é–‹å§‹è™•ç† Q{qid}...", log_callback)
    q_text = col.split(":", 1)[1].strip()
    need_score = qid in TARGET_SCORE_Q
    rubric = RUBRICS.get(qid, "")
    
    sub = df[["name", col]].rename(columns={col: "answer"}).fillna("")
    sub[f"Q{qid}_sim_flag"] = calculate_similarity_flags(sub["answer"].tolist())
    
    out_ai, out_sc = [], []
    chunks = [sub.iloc[i:i + BATCH_SIZE] for i in range(0, len(sub), BATCH_SIZE)]
    
    for i, chunk in enumerate(chunks):
        log_to_frontend(f"    Q{qid}: è™•ç†æ‰¹æ¬¡ {i+1}/{len(chunks)}", log_callback)
        tasks = [gemini_eval(q_text, rubric, a, need_score) for a in chunk["answer"]]
        results = await asyncio.gather(*tasks)
        out_ai.extend([r[0] for r in results])
        out_sc.extend([r[1] for r in results])
        
    sub[f"Q{qid}_ai_risk"] = out_ai
    if need_score:
        sub[f"Q{qid}_score"] = out_sc
        
    return sub.drop(columns=["answer"])

async def run_analysis(api_key: str, csv_path: str, out_path: str, log_callback):
    """åŸ·è¡Œå®Œæ•´åˆ†ææµç¨‹çš„ä¸»å‡½å¼"""
    if not configure_gemini(api_key):
        log_to_frontend("âŒ API é‡‘é‘°è¨­å®šå¤±æ•—ï¼Œè«‹æª¢æŸ¥é‡‘é‘°æ˜¯å¦æ­£ç¢ºã€‚", log_callback)
        return

    df, qmap = load_exam(csv_path, log_callback)
    if df is None: return

    merged_df = df[["name"]].copy()
    
    for qid, col in qmap.items():
        res_df = await process_question(df, qid, col, log_callback)
        merged_df = merged_df.merge(res_df, on="name", how="left")
        
    try:
        merged_df.to_excel(out_path, index=False, engine='openpyxl')
        log_to_frontend(f"ğŸ‰ åˆ†æå®Œæˆï¼å ±å‘Šå·²å„²å­˜è‡³ï¼š\n{out_path}")
    except Exception as e:
        log_to_frontend(f"âŒ å„²å­˜ Excel å ±å‘Šå¤±æ•—: {e}", log_callback)