#!/usr/bin/env python3
# enhanced_plagiarism_detector.py
# å¢å¼·ç‰ˆæŠ„è¥²æª¢æ¸¬å™¨ - çµåˆå¤šç¨®æ–¹æ³•çš„æœ€ä½³å¯¦è¸

import numpy as np
import re
import logging
from typing import List, Dict, Tuple, Optional
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai

class EnhancedPlagiarismDetector:
    """
    å¢å¼·ç‰ˆæŠ„è¥²æª¢æ¸¬å™¨
    çµåˆ Google Embeddingsã€TF-IDFã€å­—ç¬¦ç›¸ä¼¼åº¦ç­‰å¤šç¨®æ–¹æ³•
    """
    
    def __init__(self, min_text_length=50, high_threshold=0.85, mid_threshold=0.70):
        self.min_text_length = min_text_length
        self.high_threshold = high_threshold
        self.mid_threshold = mid_threshold
        
    def preprocess_text(self, text: str) -> str:
        """æ¨™æº–åŒ–æ–‡æœ¬é è™•ç†"""
        if not text or not text.strip():
            return ""
        
        # 1. åŸºæœ¬æ¸…ç†
        cleaned = text.strip()
        
        # 2. çµ±ä¸€ç©ºç™½å­—ç¬¦
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        # 3. ç§»é™¤å¤šé¤˜æ¨™é»ï¼ˆå¯é¸ï¼‰
        # cleaned = re.sub(r'[^\w\s]', ' ', cleaned)
        
        # 4. è½‰æ›ç‚ºå°å¯«ï¼ˆç”¨æ–¼æŸäº›æ¯”è¼ƒï¼‰
        # cleaned = cleaned.lower()
        
        return cleaned
    
    def calculate_embedding_similarity(self, texts: List[str], names: List[str] = None) -> Tuple[List[int], List[Dict]]:
        """
        ä½¿ç”¨ Google Embeddings è¨ˆç®—èªæ„ç›¸ä¼¼åº¦
        """
        try:
            # é è™•ç†
            processed_texts = [self.preprocess_text(t) for t in texts]
            
            # éæ¿¾éçŸ­æ–‡æœ¬
            valid_mask = [len(t) >= self.min_text_length for t in processed_texts]
            if sum(valid_mask) < 2:
                return [0] * len(texts), []
            
            # è¨ˆç®—åµŒå…¥å‘é‡
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=processed_texts,
                task_type="RETRIEVAL_DOCUMENT"
            )
            embeddings = result['embedding']
            
            # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
            sim_matrix = cosine_similarity(embeddings)
            np.fill_diagonal(sim_matrix, 0)
            
            # åˆ†ææ¯å€‹å­¸ç”Ÿçš„æœ€é«˜ç›¸ä¼¼åº¦
            flags = []
            details = []
            
            for i in range(len(texts)):
                if not valid_mask[i]:
                    flags.append(0)
                    continue
                
                max_sim = np.max(sim_matrix[i])
                max_idx = np.argmax(sim_matrix[i])
                
                # åˆ†é¡ç›¸ä¼¼åº¦ç­‰ç´š
                if max_sim >= self.high_threshold:
                    flag = 2
                elif max_sim >= self.mid_threshold:
                    flag = 1
                else:
                    flag = 0
                
                flags.append(flag)
                
                # è¨˜éŒ„é«˜ç›¸ä¼¼åº¦è©³æƒ…
                if max_sim >= self.mid_threshold:
                    student_i = names[i] if names else f"å­¸ç”Ÿ {i+1}"
                    student_j = names[max_idx] if names else f"å­¸ç”Ÿ {max_idx+1}"
                    details.append({
                        "method": "embedding",
                        "student_1": student_i,
                        "student_2": student_j,
                        "similarity": float(max_sim),
                        "level": "high" if flag == 2 else "medium",
                        "index_1": i,
                        "index_2": max_idx
                    })
            
            return flags, details
            
        except Exception as e:
            logging.error(f"Embedding ç›¸ä¼¼åº¦è¨ˆç®—éŒ¯èª¤: {e}")
            return [-1] * len(texts), []
    
    def calculate_tfidf_similarity(self, texts: List[str], names: List[str] = None) -> Tuple[List[int], List[Dict]]:
        """
        ä½¿ç”¨ TF-IDF è¨ˆç®—è©å½™ç›¸ä¼¼åº¦
        """
        try:
            if len(texts) < 2:
                return [0] * len(texts), []
            
            # é è™•ç†
            processed_texts = [self.preprocess_text(t) if t else " " for t in texts]
            
            # TF-IDF å‘é‡åŒ–
            vectorizer = TfidfVectorizer(
                stop_words='english',
                max_features=1000,
                ngram_range=(1, 2),
                min_df=1
            )
            tfidf_matrix = vectorizer.fit_transform(processed_texts)
            
            # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
            sim_matrix = cosine_similarity(tfidf_matrix)
            np.fill_diagonal(sim_matrix, 0)
            
            # åˆ†æçµæœ
            flags = []
            details = []
            
            for i in range(len(texts)):
                if len(processed_texts[i].strip()) < self.min_text_length:
                    flags.append(0)
                    continue
                
                max_sim = np.max(sim_matrix[i])
                max_idx = np.argmax(sim_matrix[i])
                
                if max_sim >= 0.9:  # TF-IDF é–¾å€¼è¼ƒé«˜
                    flag = 2
                elif max_sim >= 0.75:
                    flag = 1
                else:
                    flag = 0
                
                flags.append(flag)
                
                if max_sim >= 0.75:
                    student_i = names[i] if names else f"å­¸ç”Ÿ {i+1}"
                    student_j = names[max_idx] if names else f"å­¸ç”Ÿ {max_idx+1}"
                    details.append({
                        "method": "tfidf",
                        "student_1": student_i,
                        "student_2": student_j,
                        "similarity": float(max_sim),
                        "level": "high" if flag == 2 else "medium",
                        "index_1": i,
                        "index_2": max_idx
                    })
            
            return flags, details
            
        except Exception as e:
            logging.error(f"TF-IDF ç›¸ä¼¼åº¦è¨ˆç®—éŒ¯èª¤: {e}")
            return [-1] * len(texts), []
    
    def calculate_exact_match(self, texts: List[str], names: List[str] = None) -> List[Dict]:
        """
        æª¢æ¸¬å®Œå…¨ç›¸åŒæˆ–å¹¾ä¹ç›¸åŒçš„ç­”æ¡ˆ
        """
        exact_matches = []
        processed_texts = [self.preprocess_text(t).lower() for t in texts]
        
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                if not processed_texts[i] or not processed_texts[j]:
                    continue
                
                # æª¢æŸ¥å®Œå…¨ç›¸åŒ
                if processed_texts[i] == processed_texts[j]:
                    student_i = names[i] if names else f"å­¸ç”Ÿ {i+1}"
                    student_j = names[j] if names else f"å­¸ç”Ÿ {j+1}"
                    exact_matches.append({
                        "method": "exact_match",
                        "student_1": student_i,
                        "student_2": student_j,
                        "similarity": 1.0,
                        "level": "identical",
                        "index_1": i,
                        "index_2": j
                    })
        
        return exact_matches
    
    def comprehensive_analysis(self, texts: List[str], names: List[str] = None) -> Dict:
        """
        ç¶œåˆåˆ†æï¼šçµåˆå¤šç¨®æª¢æ¸¬æ–¹æ³•
        """
        print("ğŸ” é–‹å§‹ç¶œåˆæŠ„è¥²æª¢æ¸¬åˆ†æ...")
        
        results = {
            "total_students": len(texts),
            "methods_used": [],
            "high_risk_students": set(),
            "medium_risk_students": set(),
            "detailed_findings": [],
            "summary": {}
        }
        
        # 1. å®Œå…¨ç›¸åŒæª¢æ¸¬
        exact_matches = self.calculate_exact_match(texts, names)
        if exact_matches:
            results["methods_used"].append("exact_match")
            results["detailed_findings"].extend(exact_matches)
            for match in exact_matches:
                results["high_risk_students"].add(match["student_1"])
                results["high_risk_students"].add(match["student_2"])
        
        # 2. Embedding ç›¸ä¼¼åº¦
        try:
            embedding_flags, embedding_details = self.calculate_embedding_similarity(texts, names)
            if any(f >= 0 for f in embedding_flags):
                results["methods_used"].append("embedding")
                results["detailed_findings"].extend(embedding_details)
                
                for i, flag in enumerate(embedding_flags):
                    student = names[i] if names else f"å­¸ç”Ÿ {i+1}"
                    if flag == 2:
                        results["high_risk_students"].add(student)
                    elif flag == 1:
                        results["medium_risk_students"].add(student)
                        
        except Exception as e:
            print(f"âš ï¸ Embedding æ–¹æ³•å¤±æ•—: {e}")
        
        # 3. TF-IDF ç›¸ä¼¼åº¦
        try:
            tfidf_flags, tfidf_details = self.calculate_tfidf_similarity(texts, names)
            if any(f >= 0 for f in tfidf_flags):
                results["methods_used"].append("tfidf")
                results["detailed_findings"].extend(tfidf_details)
                
                for i, flag in enumerate(tfidf_flags):
                    student = names[i] if names else f"å­¸ç”Ÿ {i+1}"
                    if flag == 2:
                        results["high_risk_students"].add(student)
                    elif flag == 1:
                        results["medium_risk_students"].add(student)
                        
        except Exception as e:
            print(f"âš ï¸ TF-IDF æ–¹æ³•å¤±æ•—: {e}")
        
        # è½‰æ› set ç‚º list
        results["high_risk_students"] = list(results["high_risk_students"])
        results["medium_risk_students"] = list(results["medium_risk_students"])
        
        # ç”Ÿæˆæ‘˜è¦
        results["summary"] = {
            "total_methods": len(results["methods_used"]),
            "exact_matches": len(exact_matches),
            "high_risk_count": len(results["high_risk_students"]),
            "medium_risk_count": len(results["medium_risk_students"]),
            "total_flagged_pairs": len(results["detailed_findings"])
        }
        
        return results
    
    def print_analysis_report(self, results: Dict):
        """
        å°å‡ºè©³ç´°çš„åˆ†æå ±å‘Š
        """
        print("\n" + "=" * 60)
        print("ğŸ“‹ ç¶œåˆæŠ„è¥²æª¢æ¸¬å ±å‘Š")
        print("=" * 60)
        
        summary = results["summary"]
        print(f"ğŸ“Š æª¢æ¸¬æ¦‚æ³:")
        print(f"  â€¢ ç¸½å­¸ç”Ÿæ•¸: {results['total_students']}")
        print(f"  â€¢ ä½¿ç”¨æ–¹æ³•: {', '.join(results['methods_used'])}")
        print(f"  â€¢ å®Œå…¨ç›¸åŒ: {summary['exact_matches']} å°")
        print(f"  â€¢ é«˜é¢¨éšªå­¸ç”Ÿ: {summary['high_risk_count']} äºº")
        print(f"  â€¢ ä¸­é¢¨éšªå­¸ç”Ÿ: {summary['medium_risk_count']} äºº")
        print(f"  â€¢ ç¸½å¯ç–‘é…å°: {summary['total_flagged_pairs']} å°")
        
        # è©³ç´°ç™¼ç¾
        if results["detailed_findings"]:
            print(f"\nğŸš¨ è©³ç´°ç™¼ç¾:")
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            sorted_findings = sorted(
                results["detailed_findings"], 
                key=lambda x: x["similarity"], 
                reverse=True
            )
            
            for finding in sorted_findings:
                method = finding["method"]
                level = finding["level"]
                sim = finding["similarity"]
                
                emoji = {"identical": "ğŸ”´", "high": "ğŸŸ ", "medium": "ğŸŸ¡"}[level]
                print(f"  {emoji} [{method.upper()}] {finding['student_1']} â†” {finding['student_2']}")
                print(f"     ç›¸ä¼¼åº¦: {sim:.3f} ({level})")
        
        # é¢¨éšªå­¸ç”Ÿåˆ—è¡¨
        if results["high_risk_students"]:
            print(f"\nğŸ¯ é«˜é¢¨éšªå­¸ç”Ÿåˆ—è¡¨:")
            for student in sorted(results["high_risk_students"]):
                print(f"  ğŸš¨ {student}")
        
        if results["medium_risk_students"]:
            print(f"\nâš ï¸ ä¸­é¢¨éšªå­¸ç”Ÿåˆ—è¡¨:")
            for student in sorted(results["medium_risk_students"]):
                print(f"  âš ï¸ {student}")
        
        if not results["high_risk_students"] and not results["medium_risk_students"]:
            print(f"\nâœ… æœªæª¢æ¸¬åˆ°æ˜é¡¯çš„æŠ„è¥²è¡Œç‚º")


def test_enhanced_detector():
    """æ¸¬è©¦å¢å¼·ç‰ˆæª¢æ¸¬å™¨"""
    
    # æ¸¬è©¦æ¡ˆä¾‹
    test_texts = [
        "The Czochralski method is a crystal growth technique where a seed crystal is dipped into molten silicon and slowly pulled upward while rotating to form a single crystal ingot.",
        "The Czochralski process is a crystal growing technique in which a seed crystal is immersed in molten silicon and gradually pulled up while rotating to create a single crystal ingot.",
        "Molecular beam epitaxy (MBE) is a thin film deposition technique that allows precise control of layer thickness and composition by evaporating materials in ultra-high vacuum.",
        "Silicon solar cells achieve efficiency around 20-25% due to their optimal band gap of 1.1 eV, making them suitable for converting solar energy into electrical energy.",
        "Gallium arsenide (GaAs) devices have different optical and electronic properties compared to silicon, including a direct bandgap that makes them efficient for optoelectronic applications."
    ]
    
    test_names = ["Alice", "Bob", "Charlie", "David", "Eve"]
    
    print("ğŸ§ª æ¸¬è©¦å¢å¼·ç‰ˆæŠ„è¥²æª¢æ¸¬å™¨")
    print("=" * 50)
    
    # å‰µå»ºæª¢æ¸¬å™¨å¯¦ä¾‹
    detector = EnhancedPlagiarismDetector(
        min_text_length=30,
        high_threshold=0.85,
        mid_threshold=0.70
    )
    
    # åŸ·è¡Œç¶œåˆåˆ†æ
    results = detector.comprehensive_analysis(test_texts, test_names)
    
    # å°å‡ºå ±å‘Š
    detector.print_analysis_report(results)
    
    return results


if __name__ == "__main__":
    # éœ€è¦å…ˆè¨­å®š Google API
    # genai.configure(api_key="your_api_key_here")
    
    test_enhanced_detector()
