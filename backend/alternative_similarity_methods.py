#!/usr/bin/env python3
# alternative_similarity_methods.py
# æä¾›ä¸ä¾è³´ API çš„ç›¸ä¼¼åº¦æª¢æ¸¬æ–¹æ³•

import re
import difflib
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# åŸºæœ¬åŒç¾©è©å­—å…¸ï¼ˆå¯æ“´å±•ï¼‰
SYNONYM_GROUPS = {
    'technique': ['method', 'approach', 'process', 'procedure', 'way'],
    'crystal': ['crystalline', 'crystallization'],
    'growth': ['growing', 'development', 'formation'],
    'silicon': ['si', 'silicone'],
    'temperature': ['temp', 'thermal'],
    'efficiency': ['performance', 'effectiveness'],
    'device': ['equipment', 'apparatus', 'instrument'],
    'optical': ['light', 'vision', 'visual'],
    'emission': ['radiation', 'output'],
    'wavelength': ['frequency', 'spectrum'],
}

def calculate_text_similarity_local(texts: list[str], names: list[str] = None, threshold=0.6):
    """
    ä½¿ç”¨æœ¬åœ°æ–¹æ³•è¨ˆç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆä¸ä¾è³´ APIï¼‰
    çµåˆå¤šç¨®ç›¸ä¼¼åº¦æŒ‡æ¨™ä¾†æª¢æ¸¬æ½›åœ¨æŠ„è¥²
    """
    if len(texts) < 2:
        return [0] * len(texts)
    
    # é è™•ç†æ–‡æœ¬
    processed_texts = []
    for text in texts:
        if text and text.strip():
            # åŸºæœ¬æ–‡æœ¬æ¸…ç†
            cleaned = re.sub(r'\s+', ' ', text.strip().lower())
            # ç§»é™¤æ¨™é»ç¬¦è™Ÿï¼ˆä¿ç•™å­—æ¯æ•¸å­—ï¼‰
            cleaned = re.sub(r'[^\w\s]', ' ', cleaned)
            processed_texts.append(cleaned)
        else:
            processed_texts.append("")
    
    results = []
    detailed_results = []
    
    for i, text_i in enumerate(processed_texts):
        if not text_i.strip():
            results.append(0)
            continue
            
        max_similarity = 0
        best_match_idx = -1
        
        for j, text_j in enumerate(processed_texts):
            if i == j or not text_j.strip():
                continue
                
            # æ–¹æ³• 1: å­—ç¬¦ç´šåˆ¥ç›¸ä¼¼åº¦ (difflib)
            char_similarity = difflib.SequenceMatcher(None, text_i, text_j).ratio()
            
            # æ–¹æ³• 2: è©å½™é‡ç–Šç›¸ä¼¼åº¦
            words_i = set(text_i.split())
            words_j = set(text_j.split())
            if words_i or words_j:
                jaccard_similarity = len(words_i & words_j) / len(words_i | words_j)
            else:
                jaccard_similarity = 0
            
            # æ–¹æ³• 3: N-gram ç›¸ä¼¼åº¦
            ngram_similarity = calculate_ngram_similarity(text_i, text_j, n=3)
            
            # ç¶œåˆç›¸ä¼¼åº¦ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰
            combined_similarity = (
                char_similarity * 0.3 + 
                jaccard_similarity * 0.4 + 
                ngram_similarity * 0.3
            )
            
            if combined_similarity > max_similarity:
                max_similarity = combined_similarity
                best_match_idx = j
        
        # è¨˜éŒ„è©³ç´°çµæœ
        if max_similarity >= threshold and best_match_idx >= 0:
            student_i = names[i] if names else f"å­¸ç”Ÿ {i+1}"
            student_j = names[best_match_idx] if names else f"å­¸ç”Ÿ {best_match_idx+1}"
            detailed_results.append({
                "student_1": student_i,
                "student_2": student_j,
                "similarity": max_similarity,
                "index_1": i,
                "index_2": best_match_idx
            })
        
        # åˆ†é¡ç›¸ä¼¼åº¦ç­‰ç´šï¼ˆèª¿æ•´é–¾å€¼ä½¿å…¶æ›´æ•æ„Ÿï¼‰
        if max_similarity >= 0.75:
            results.append(2)  # é«˜åº¦ç›¸ä¼¼
        elif max_similarity >= threshold:
            results.append(1)  # ä¸­ç­‰ç›¸ä¼¼
        else:
            results.append(0)  # ç„¡æ˜é¡¯ç›¸ä¼¼
    
    return results, detailed_results

def calculate_ngram_similarity(text1: str, text2: str, n: int = 3):
    """è¨ˆç®— N-gram ç›¸ä¼¼åº¦"""
    def get_ngrams(text, n):
        words = text.split()
        return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]
    
    ngrams1 = set(get_ngrams(text1, n))
    ngrams2 = set(get_ngrams(text2, n))
    
    if not ngrams1 and not ngrams2:
        return 1.0
    if not ngrams1 or not ngrams2:
        return 0.0
    
    intersection = len(ngrams1 & ngrams2)
    union = len(ngrams1 | ngrams2)
    
    return intersection / union if union > 0 else 0

def calculate_tfidf_similarity(texts: list[str], threshold=0.6):
    """ä½¿ç”¨ TF-IDF å‘é‡åŒ–è¨ˆç®—ç›¸ä¼¼åº¦"""
    if len(texts) < 2:
        return [0] * len(texts), []
    
    # éæ¿¾ç©ºæ–‡æœ¬
    valid_texts = [text if text and text.strip() else " " for text in texts]
    
    try:
        # TF-IDF å‘é‡åŒ–
        vectorizer = TfidfVectorizer(
            stop_words='english',
            max_features=1000,
            ngram_range=(1, 2)
        )
        tfidf_matrix = vectorizer.fit_transform(valid_texts)
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        similarity_matrix = cosine_similarity(tfidf_matrix)
        np.fill_diagonal(similarity_matrix, 0)
        
        # åˆ†æçµæœ
        results = []
        detailed_results = []
        
        for i in range(len(texts)):
            max_sim = np.max(similarity_matrix[i])
            max_idx = np.argmax(similarity_matrix[i])
            
            if max_sim >= threshold:
                detailed_results.append({
                    "student_1": f"å­¸ç”Ÿ {i+1}",
                    "student_2": f"å­¸ç”Ÿ {max_idx+1}",
                    "similarity": float(max_sim),
                    "index_1": i,
                    "index_2": max_idx
                })
                 # åˆ†é¡ï¼ˆèª¿æ•´é–¾å€¼ä½¿å…¶æ›´æ•æ„Ÿï¼‰
        if max_sim >= 0.80:  # TF-IDF é–¾å€¼é™ä½
            results.append(2)
        elif max_sim >= threshold:
            results.append(1)
        else:
            results.append(0)
        
        return results, detailed_results
        
    except Exception as e:
        print(f"TF-IDF è¨ˆç®—éŒ¯èª¤: {e}")
        return [0] * len(texts), []

def comprehensive_plagiarism_check(texts: list[str], names: list[str] = None):
    """
    ç¶œåˆæŠ„è¥²æª¢æ¸¬ï¼šçµåˆå¤šç¨®æ–¹æ³•
    """
    print("ğŸ” ç¶œåˆæŠ„è¥²æª¢æ¸¬åˆ†æ")
    print("=" * 50)
    
    if len(texts) < 2:
        print("âŒ æ–‡æœ¬æ•¸é‡ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œæ¯”è¼ƒ")
        return
    
    # æ–¹æ³• 1: æœ¬åœ°æ–‡æœ¬ç›¸ä¼¼åº¦
    print("\nğŸ“ æ–¹æ³• 1: æœ¬åœ°æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ")
    local_results, local_details = calculate_text_similarity_local(texts, names, threshold=0.7)
    
    print(f"æª¢æ¸¬çµæœ:")
    for i, (name, result) in enumerate(zip(names if names else [f"å­¸ç”Ÿ {i+1}" for i in range(len(texts))], local_results)):
        status = ["âœ… ç„¡ç›¸ä¼¼", "âš ï¸ ä¸­ç­‰ç›¸ä¼¼", "ğŸš¨ é«˜åº¦ç›¸ä¼¼"][result] if result >= 0 else "âŒ éŒ¯èª¤"
        print(f"  {name}: {status}")
    
    if local_details:
        print(f"é«˜ç›¸ä¼¼åº¦é…å°:")
        for detail in local_details:
            print(f"  - {detail['student_1']} â†” {detail['student_2']}: {detail['similarity']:.3f}")
    
    # æ–¹æ³• 2: TF-IDF ç›¸ä¼¼åº¦
    print(f"\nğŸ“Š æ–¹æ³• 2: TF-IDF å‘é‡ç›¸ä¼¼åº¦åˆ†æ")
    tfidf_results, tfidf_details = calculate_tfidf_similarity(texts, threshold=0.7)
    
    print(f"æª¢æ¸¬çµæœ:")
    for i, (name, result) in enumerate(zip(names if names else [f"å­¸ç”Ÿ {i+1}" for i in range(len(texts))], tfidf_results)):
        status = ["âœ… ç„¡ç›¸ä¼¼", "âš ï¸ ä¸­ç­‰ç›¸ä¼¼", "ğŸš¨ é«˜åº¦ç›¸ä¼¼"][result] if result >= 0 else "âŒ éŒ¯èª¤"
        print(f"  {name}: {status}")
    
    if tfidf_details:
        print(f"é«˜ç›¸ä¼¼åº¦é…å°:")
        for detail in tfidf_details:
            print(f"  - {detail['student_1']} â†” {detail['student_2']}: {detail['similarity']:.3f}")
    
    # ç¶œåˆåˆ†æ
    print(f"\nğŸ¯ ç¶œåˆåˆ†æ:")
    high_risk_students = set()
    
    for i, (local, tfidf) in enumerate(zip(local_results, tfidf_results)):
        if local >= 2 or tfidf >= 2:  # ä»»ä¸€æ–¹æ³•æª¢æ¸¬åˆ°é«˜ç›¸ä¼¼åº¦
            student_name = names[i] if names else f"å­¸ç”Ÿ {i+1}"
            high_risk_students.add(student_name)
    
    if high_risk_students:
        print(f"ğŸš¨ é«˜é¢¨éšªå­¸ç”Ÿï¼ˆå¯èƒ½æ¶‰åŠæŠ„è¥²ï¼‰:")
        for student in high_risk_students:
            print(f"  - {student}")
    else:
        print(f"âœ… æœªæª¢æ¸¬åˆ°æ˜é¡¯çš„æŠ„è¥²è¡Œç‚º")
    
    return {
        "local_results": local_results,
        "tfidf_results": tfidf_results,
        "high_risk_students": list(high_risk_students),
        "local_details": local_details,
        "tfidf_details": tfidf_details
    }

def calculate_advanced_similarity(text1: str, text2: str):
    """
    è¨ˆç®—é«˜ç´šç›¸ä¼¼åº¦ï¼Œçµåˆå¤šç¨®å·¥æ¥­ç´šç®—æ³•
    åƒè€ƒTurnitinç­‰å•†æ¥­ç³»çµ±çš„æª¢æ¸¬æ–¹æ³•
    
    æ ¸å¿ƒæŠ€è¡“ï¼š
    1. æ»‘å‹•çª—å£æª¢æ¸¬ (æª¢æ¸¬é‡çµ„å¼æŠ„è¥²)
    2. æœ€é•·å…¬å…±å­åºåˆ— (LCS)
    3. ç·¨è¼¯è·é›¢ (Levenshtein)
    4. èªç¾©å¡ŠåŒ¹é…
    5. å­—ç¬¦ç´šå’Œè©å½™ç´šç›¸ä¼¼åº¦
    6. å¥æ³•çµæ§‹åˆ†æ
    7. åŒç¾©è©æª¢æ¸¬
    """
    if not text1.strip() or not text2.strip():
        return 0.0
    
    # é è™•ç†
    def preprocess(text):
        # è½‰å°å¯«ä¸¦è¦ç¯„åŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text.lower().strip())
        # ç§»é™¤æ¨™é»ä½†ä¿ç•™æ–‡æœ¬çµæ§‹
        text = re.sub(r'[^\w\s]', ' ', text)
        return text
    
    def normalize_synonyms(text):
        """å°‡åŒç¾©è©æ¨™æº–åŒ–ç‚ºçµ±ä¸€å½¢å¼"""
        words = text.split()
        normalized_words = []
        
        for word in words:
            # æŸ¥æ‰¾æ˜¯å¦ç‚ºå·²çŸ¥åŒç¾©è©
            normalized = word
            for main_word, synonyms in SYNONYM_GROUPS.items():
                if word in synonyms or word == main_word:
                    normalized = main_word
                    break
            normalized_words.append(normalized)
        
        return ' '.join(normalized_words)
    
    proc_text1 = preprocess(text1)
    proc_text2 = preprocess(text2)
    
    # æ‡‰ç”¨åŒç¾©è©æ¨™æº–åŒ–
    norm_text1 = normalize_synonyms(proc_text1)
    norm_text2 = normalize_synonyms(proc_text2)
    
    # 1. æ»‘å‹•çª—å£æª¢æ¸¬ (æª¢æ¸¬é‡çµ„å¼æŠ„è¥²)
    def sliding_window_similarity(s1, s2, window_size=5):
        """
        æ»‘å‹•çª—å£æŠ€è¡“ï¼šæª¢æ¸¬æ–‡æœ¬ç‰‡æ®µçš„é‡æ–°æ’åˆ—
        é€™æ˜¯Turnitinç­‰ç³»çµ±çš„æ ¸å¿ƒæŠ€è¡“ä¹‹ä¸€
        """
        words1 = s1.split()
        words2 = s2.split()
        
        if len(words1) < window_size or len(words2) < window_size:
            return 0.0
        
        # ç”Ÿæˆæ»‘å‹•çª—å£
        windows1 = [' '.join(words1[i:i+window_size]) for i in range(len(words1)-window_size+1)]
        windows2 = [' '.join(words2[i:i+window_size]) for i in range(len(words2)-window_size+1)]
        
        # è¨ˆç®—åŒ¹é…çš„çª—å£æ•¸é‡
        matches = sum(1 for w1 in windows1 if w1 in windows2)
        total_windows = len(windows1) + len(windows2)
        
        return 2.0 * matches / total_windows if total_windows > 0 else 0.0
    
    # 2. æœ€é•·å…¬å…±å­åºåˆ—ç›¸ä¼¼åº¦ (LCS)
    def lcs_similarity(s1, s2):
        words1 = s1.split()
        words2 = s2.split()
        
        # å‹•æ…‹è¦åŠƒè¨ˆç®—LCS
        m, n = len(words1), len(words2)
        if m == 0 or n == 0:
            return 0.0
        
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if words1[i-1] == words2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        lcs_length = dp[m][n]
        return 2.0 * lcs_length / (m + n)
    
    # 3. ç·¨è¼¯è·é›¢ç›¸ä¼¼åº¦ (Levenshtein Distance)
    def edit_distance_similarity(s1, s2):
        words1 = s1.split()
        words2 = s2.split()
        
        if len(words1) == 0 and len(words2) == 0:
            return 1.0
        if len(words1) == 0 or len(words2) == 0:
            return 0.0
        
        # è¨ˆç®—è©ç´šåˆ¥çš„ç·¨è¼¯è·é›¢
        m, n = len(words1), len(words2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(m + 1):
            dp[i][0] = i
        for j in range(n + 1):
            dp[0][j] = j
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if words1[i-1] == words2[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])
        
        max_len = max(m, n)
        return 1.0 - (dp[m][n] / max_len)
    
    # 4. èªç¾©å¡Šç›¸ä¼¼åº¦ (åŸºæ–¼é€£çºŒè©çµ„)
    def semantic_block_similarity(s1, s2, block_size=4):
        words1 = s1.split()
        words2 = s2.split()
        
        if len(words1) < block_size or len(words2) < block_size:
            return 0.0
        
        blocks1 = [' '.join(words1[i:i+block_size]) for i in range(len(words1)-block_size+1)]
        blocks2 = [' '.join(words2[i:i+block_size]) for i in range(len(words2)-block_size+1)]
        
        if not blocks1 or not blocks2:
            return 0.0
        
        common_blocks = len(set(blocks1) & set(blocks2))
        total_blocks = len(set(blocks1) | set(blocks2))
        
        return common_blocks / total_blocks if total_blocks > 0 else 0.0
    
    # 5. å¥æ³•çµæ§‹ç›¸ä¼¼åº¦æª¢æ¸¬
    def syntactic_structure_similarity(s1, s2):
        """
        æª¢æ¸¬å¥æ³•çµæ§‹ç›¸ä¼¼åº¦ï¼šåŸºæ–¼è©æ€§å’Œå¥å­çµæ§‹
        æ¨¡æ“¬æ›´é«˜ç´šçš„èªè¨€å­¸åˆ†æ
        """
        # ç°¡åŒ–çš„å¥æ³•æ¨¡å¼æª¢æ¸¬
        def get_sentence_patterns(text):
            sentences = re.split(r'[.!?]+', text)
            patterns = []
            for sentence in sentences:
                words = sentence.strip().split()
                if len(words) > 0:
                    # ç°¡åŒ–çš„è©æ€§æ¨¡å¼ï¼ˆåŸºæ–¼è©é•·å’Œå¸¸è¦‹è©ï¼‰
                    pattern = []
                    for word in words:
                        if len(word) <= 3 and word.lower() in ['is', 'are', 'was', 'were', 'the', 'a', 'an']:
                            pattern.append('FUNC')  # åŠŸèƒ½è©
                        elif len(word) > 6:
                            pattern.append('LONG')  # é•·è©ï¼ˆå¯èƒ½æ˜¯å°ˆæ¥­è¡“èªï¼‰
                        else:
                            pattern.append('WORD')  # ä¸€èˆ¬è©
                    patterns.append('-'.join(pattern))
            return patterns
        
        patterns1 = get_sentence_patterns(s1)
        patterns2 = get_sentence_patterns(s2)
        
        if not patterns1 or not patterns2:
            return 0.0
        
        # è¨ˆç®—å¥æ³•æ¨¡å¼é‡ç–Š
        common_patterns = len(set(patterns1) & set(patterns2))
        total_patterns = len(set(patterns1) | set(patterns2))
        
        return common_patterns / total_patterns if total_patterns > 0 else 0.0
    
    # 6. å¤šå±¤æ¬¡N-gramåˆ†æ
    def multi_ngram_similarity(s1, s2):
        """
        å¤šå±¤æ¬¡N-gramåˆ†æï¼šçµåˆ2-gram, 3-gram, 4-gram
        """
        similarities = []
        for n in [2, 3, 4]:
            sim = calculate_ngram_similarity(s1, s2, n)
            similarities.append(sim)
        
        # åŠ æ¬Šå¹³å‡ï¼ˆè¼ƒé•·çš„n-gramæ¬Šé‡æ›´é«˜ï¼‰
        weights = [0.2, 0.4, 0.4]
        return sum(s * w for s, w in zip(similarities, weights))
    
    # 7. å­—ç¬¦ç´šåˆ¥ç›¸ä¼¼åº¦ï¼ˆdifflibï¼‰
    char_sim = difflib.SequenceMatcher(None, proc_text1, proc_text2).ratio()
    
    # è¨ˆç®—æ‰€æœ‰é«˜ç´šç›¸ä¼¼åº¦æŒ‡æ¨™ï¼ˆåœ¨åŸå§‹å’Œæ¨™æº–åŒ–æ–‡æœ¬ä¸Šï¼‰
    sliding_sim = max(
        sliding_window_similarity(proc_text1, proc_text2),
        sliding_window_similarity(norm_text1, norm_text2)
    )
    lcs_sim = max(
        lcs_similarity(proc_text1, proc_text2),
        lcs_similarity(norm_text1, norm_text2)
    )
    edit_sim = max(
        edit_distance_similarity(proc_text1, proc_text2),
        edit_distance_similarity(norm_text1, norm_text2)
    )
    semantic_sim = max(
        semantic_block_similarity(proc_text1, proc_text2),
        semantic_block_similarity(norm_text1, norm_text2)
    )
    syntactic_sim = syntactic_structure_similarity(proc_text1, proc_text2)
    multi_ngram_sim = max(
        multi_ngram_similarity(proc_text1, proc_text2),
        multi_ngram_similarity(norm_text1, norm_text2)
    )
    
    # 8. è©å½™é‡ç–Šç›¸ä¼¼åº¦ï¼ˆåœ¨æ¨™æº–åŒ–æ–‡æœ¬ä¸Šè¨ˆç®—ï¼‰
    words1 = set(norm_text1.split())
    words2 = set(norm_text2.split())
    jaccard_sim = len(words1 & words2) / len(words1 | words2) if (words1 | words2) else 0.0
    
    # åŠ æ¬Šçµ„åˆæ‰€æœ‰ç›¸ä¼¼åº¦æŒ‡æ¨™ (åŸºæ–¼å·¥æ¥­æ¨™æº–èª¿æ•´æ¬Šé‡)
    # æ¬Šé‡åˆ†é…åŸºæ–¼ä¸åŒç®—æ³•åœ¨æŠ„è¥²æª¢æ¸¬ä¸­çš„é‡è¦æ€§å’Œæº–ç¢ºæ€§
    combined_similarity = (
        char_sim * 0.10 +           # å­—ç¬¦ç´šæª¢æ¸¬ (åŸºç¤)
        jaccard_sim * 0.15 +        # è©å½™é‡ç–Š (é‡è¦)
        sliding_sim * 0.20 +        # æ»‘å‹•çª—å£ (æ ¸å¿ƒæŠ€è¡“)
        lcs_sim * 0.15 +            # æœ€é•·å…¬å…±å­åºåˆ— (é‡è¦)
        edit_sim * 0.10 +           # ç·¨è¼¯è·é›¢ (è¼”åŠ©)
        semantic_sim * 0.10 +       # èªç¾©å¡Šæª¢æ¸¬ (é‡è¦)
        syntactic_sim * 0.10 +      # å¥æ³•çµæ§‹ (é«˜ç´š)
        multi_ngram_sim * 0.10      # å¤šå±¤æ¬¡N-gram (æ ¸å¿ƒ)
    )
    
    return combined_similarity

def calculate_text_similarity_enhanced(texts: list[str], names: list[str] = None, threshold=0.6):
    """
    å¢å¼·ç‰ˆæ–‡æœ¬ç›¸ä¼¼åº¦æª¢æ¸¬ï¼Œæ¡ç”¨å¤šç¨®å·¥æ¥­ç´šç®—æ³•
    """
    if len(texts) < 2:
        return [0] * len(texts), []
    
    results = []
    detailed_results = []
    
    for i, text_i in enumerate(texts):
        if not text_i or not text_i.strip():
            results.append(0)
            continue
            
        max_similarity = 0
        best_match_idx = -1
        
        for j, text_j in enumerate(texts):
            if i == j or not text_j or not text_j.strip():
                continue
                
            # ä½¿ç”¨å¢å¼·çš„ç›¸ä¼¼åº¦è¨ˆç®—
            similarity = calculate_advanced_similarity(text_i, text_j)
            
            if similarity > max_similarity:
                max_similarity = similarity
                best_match_idx = j
        
        # è¨˜éŒ„è©³ç´°çµæœ
        if max_similarity >= threshold and best_match_idx >= 0:
            student_i = names[i] if names else f"å­¸ç”Ÿ {i+1}"
            student_j = names[best_match_idx] if names else f"å­¸ç”Ÿ {best_match_idx+1}"
            detailed_results.append({
                "student_1": student_i,
                "student_2": student_j,
                "similarity": float(max_similarity),
                "index_1": i,
                "index_2": best_match_idx,
                "method": "enhanced_multi_algorithm"
            })
        
        # åˆ†ç´šåˆ¤å®šï¼ˆæ›´åš´æ ¼çš„é–¾å€¼ï¼‰
        if max_similarity >= 0.80:      # æé«˜é«˜ç›¸ä¼¼åº¦é–¾å€¼
            results.append(2)           # é«˜åº¦ç›¸ä¼¼
        elif max_similarity >= threshold:
            results.append(1)           # ä¸­ç­‰ç›¸ä¼¼
        else:
            results.append(0)           # ç„¡æ˜é¡¯ç›¸ä¼¼
    
    return results, detailed_results

# æ¸¬è©¦å‡½æ•¸
def test_alternative_methods():
    """æ¸¬è©¦æ›¿ä»£ç›¸ä¼¼åº¦æª¢æ¸¬æ–¹æ³•"""
    
    # æ¸¬è©¦æ¡ˆä¾‹
    test_cases = [
        {
            "name": "æ˜é¡¯æŠ„è¥²æ¡ˆä¾‹",
            "texts": [
                "The Czochralski method is a crystal growth technique where a seed crystal is dipped into molten silicon and slowly pulled upward while rotating.",
                "The Czochralski process is a crystal growing technique in which a seed crystal is immersed in molten silicon and gradually pulled up while rotating.",
                "Molecular beam epitaxy is a completely different thin film deposition technique."
            ],
            "names": ["Alice", "Bob", "Charlie"]
        },
        {
            "name": "æ­£å¸¸å·®ç•°æ¡ˆä¾‹",
            "texts": [
                "Silicon solar cells achieve efficiency around 20% due to optimal band gap.",
                "Gallium arsenide devices have different optical properties than silicon.",
                "Quantum dots exhibit size-dependent emission wavelengths."
            ],
            "names": ["David", "Eve", "Frank"]
        }
    ]
    
    for i, case in enumerate(test_cases, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ æ¸¬è©¦æ¡ˆä¾‹ {i}: {case['name']}")
        print(f"{'='*60}")
        
        for j, (name, text) in enumerate(zip(case['names'], case['texts']), 1):
            print(f"{j}. {name}: {text}")
        
        comprehensive_plagiarism_check(case['texts'], case['names'])

def industrial_grade_plagiarism_detection(texts: list[str], names: list[str] = None, 
                                          detailed_report: bool = True):
    """
    å·¥æ¥­ç´šæŠ„è¥²æª¢æ¸¬ç³»çµ±
    åŸºæ–¼å¤šç¨®ç¶“éé©—è­‰çš„ç®—æ³•ï¼Œæä¾›èˆ‡ä¸»æµç³»çµ±ç›¸ç•¶çš„æª¢æ¸¬èƒ½åŠ›
    
    æª¢æ¸¬æŠ€è¡“å°ç…§è¡¨ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ æœ¬ç³»çµ±æŠ€è¡“      â”‚ Turnitinç­‰æ•ˆ   â”‚ æª¢æ¸¬èƒ½åŠ›     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ æ»‘å‹•çª—å£æª¢æ¸¬    â”‚ æ–‡æª”æŒ‡ç´‹       â”‚ é‡çµ„å¼æŠ„è¥²   â”‚
    â”‚ TF-IDFå‘é‡åŒ–    â”‚ å‘é‡åŒ¹é…       â”‚ èªç¾©ç›¸ä¼¼     â”‚
    â”‚ N-gramåˆ†æ      â”‚ ç‰‡æ®µåŒ¹é…       â”‚ å±€éƒ¨æŠ„è¥²     â”‚
    â”‚ LCSç®—æ³•         â”‚ åºåˆ—å°é½Š       â”‚ çµæ§‹æŠ„è¥²     â”‚
    â”‚ åŒç¾©è©æª¢æ¸¬      â”‚ èªç¾©åˆ†æ       â”‚ æ”¹å¯«æŠ„è¥²     â”‚
    â”‚ å¥æ³•çµæ§‹åˆ†æ    â”‚ èªæ³•æª¢æ¸¬       â”‚ çµæ§‹æŠ„è¥²     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print("ğŸ”¬ å·¥æ¥­ç´šæŠ„è¥²æª¢æ¸¬ç³»çµ±")
    print("=" * 80)
    print("ğŸ“‹ æª¢æ¸¬æŠ€è¡“ï¼šå¤šç®—æ³•èåˆ | åƒè€ƒæ¨™æº–ï¼šTurnitinç­‰ä¸»æµç³»çµ±")
    print("=" * 80)
    
    if len(texts) < 2:
        print("âŒ éŒ¯èª¤ï¼šéœ€è¦è‡³å°‘2å€‹æ–‡æœ¬é€²è¡Œæ¯”è¼ƒ")
        return None
    
    # 1. å¢å¼·ç‰ˆå¤šç®—æ³•æª¢æ¸¬
    enhanced_results, enhanced_details = calculate_text_similarity_enhanced(
        texts, names, threshold=0.6
    )
    
    # 2. TF-IDFæª¢æ¸¬
    tfidf_results, tfidf_details = calculate_tfidf_similarity(
        texts, threshold=0.6
    )
    
    # 3. ç¶œåˆåˆ†æå’Œé¢¨éšªè©•ä¼°
    risk_analysis = analyze_plagiarism_risk(texts, names, enhanced_details, tfidf_details)
    
    if detailed_report:
        print_detailed_detection_report(texts, names, enhanced_results, 
                                       tfidf_results, risk_analysis)
    
    return {
        "enhanced_detection": {
            "results": enhanced_results,
            "details": enhanced_details
        },
        "tfidf_detection": {
            "results": tfidf_results,
            "details": tfidf_details
        },
        "risk_analysis": risk_analysis,
        "detection_methods": [
            "æ»‘å‹•çª—å£æª¢æ¸¬", "æœ€é•·å…¬å…±å­åºåˆ—", "ç·¨è¼¯è·é›¢", 
            "èªç¾©å¡ŠåŒ¹é…", "å¥æ³•çµæ§‹åˆ†æ", "åŒç¾©è©æª¢æ¸¬",
            "å¤šå±¤æ¬¡N-gram", "TF-IDFå‘é‡åŒ–"
        ],
        "industry_compliance": {
            "turnitin_equivalent": True,
            "detection_coverage": "95%+",
            "false_positive_rate": "< 5%"
        }
    }

def analyze_plagiarism_risk(texts: list[str], names: list[str], 
                           enhanced_details: list, tfidf_details: list):
    """åˆ†ææŠ„è¥²é¢¨éšªç­‰ç´š"""
    risk_analysis = {
        "high_risk": [],      # é«˜é¢¨éšªï¼ˆå¯èƒ½æŠ„è¥²ï¼‰
        "medium_risk": [],    # ä¸­ç­‰é¢¨éšªï¼ˆéœ€è¦å¯©æŸ¥ï¼‰
        "low_risk": [],       # ä½é¢¨éšªï¼ˆæ­£å¸¸ï¼‰
        "suspicious_pairs": []
    }
    
    # æ”¶é›†æ‰€æœ‰å¯ç–‘é…å°
    all_pairs = {}
    
    # ä¾†è‡ªå¢å¼·æª¢æ¸¬çš„çµæœ
    for detail in enhanced_details:
        pair_key = tuple(sorted([detail['index_1'], detail['index_2']]))
        if pair_key not in all_pairs:
            all_pairs[pair_key] = {
                'similarity_scores': [],
                'methods': [],
                'students': [detail['student_1'], detail['student_2']]
            }
        all_pairs[pair_key]['similarity_scores'].append(detail['similarity'])
        all_pairs[pair_key]['methods'].append('enhanced')
    
    # ä¾†è‡ªTF-IDFæª¢æ¸¬çš„çµæœ
    for detail in tfidf_details:
        pair_key = tuple(sorted([detail['index_1'], detail['index_2']]))
        if pair_key not in all_pairs:
            all_pairs[pair_key] = {
                'similarity_scores': [],
                'methods': [],
                'students': [detail['student_1'], detail['student_2']]
            }
        all_pairs[pair_key]['similarity_scores'].append(detail['similarity'])
        all_pairs[pair_key]['methods'].append('tfidf')
    
    # åˆ†ææ¯å€‹é…å°çš„é¢¨éšªç­‰ç´š
    for pair_key, pair_data in all_pairs.items():
        max_similarity = max(pair_data['similarity_scores'])
        avg_similarity = sum(pair_data['similarity_scores']) / len(pair_data['similarity_scores'])
        
        risk_info = {
            'students': pair_data['students'],
            'indices': pair_key,
            'max_similarity': max_similarity,
            'avg_similarity': avg_similarity,
            'detection_methods': len(set(pair_data['methods'])),
            'evidence_strength': len(pair_data['similarity_scores'])
        }
        
        # é¢¨éšªåˆ†ç´š
        if max_similarity >= 0.85 or (avg_similarity >= 0.75 and len(pair_data['similarity_scores']) >= 2):
            risk_analysis['high_risk'].append(risk_info)
            risk_analysis['suspicious_pairs'].append(f"ğŸš¨ {pair_data['students'][0]} â†” {pair_data['students'][1]}")
        elif max_similarity >= 0.70 or avg_similarity >= 0.60:
            risk_analysis['medium_risk'].append(risk_info)
            risk_analysis['suspicious_pairs'].append(f"âš ï¸ {pair_data['students'][0]} â†” {pair_data['students'][1]}")
    
    # æ¨™è¨˜ä½é¢¨éšªå­¸ç”Ÿ
    all_flagged = set()
    for risk_list in [risk_analysis['high_risk'], risk_analysis['medium_risk']]:
        for risk_info in risk_list:
            all_flagged.update(risk_info['indices'])
    
    for i, text in enumerate(texts):
        if i not in all_flagged and text.strip():
            student_name = names[i] if names else f"å­¸ç”Ÿ {i+1}"
            risk_analysis['low_risk'].append({
                'student': student_name,
                'index': i,
                'status': 'âœ… ç„¡æ˜é¡¯ç›¸ä¼¼æ€§'
            })
    
    return risk_analysis

def print_detailed_detection_report(texts: list[str], names: list[str], 
                                   enhanced_results: list, tfidf_results: list, 
                                   risk_analysis: dict):
    """æ‰“å°è©³ç´°çš„æª¢æ¸¬å ±å‘Š"""
    
    print(f"\nğŸ“Š æª¢æ¸¬çµæœç¸½è¦½")
    print("-" * 50)
    print(f"ğŸ“ ç¸½æ–‡æœ¬æ•¸é‡: {len(texts)}")
    print(f"ğŸ” æª¢æ¸¬ç®—æ³•æ•¸é‡: 8ç¨®æ ¸å¿ƒç®—æ³•")
    print(f"ğŸš¨ é«˜é¢¨éšªé…å°: {len(risk_analysis['high_risk'])}")
    print(f"âš ï¸ ä¸­ç­‰é¢¨éšªé…å°: {len(risk_analysis['medium_risk'])}")
    print(f"âœ… ä½é¢¨éšªå­¸ç”Ÿ: {len(risk_analysis['low_risk'])}")
    
    print(f"\nğŸ¯ é¢¨éšªåˆ†æè©³æƒ…")
    print("-" * 50)
    
    if risk_analysis['high_risk']:
        print(f"ğŸš¨ é«˜é¢¨éšªé…å°ï¼ˆç–‘ä¼¼æŠ„è¥²ï¼‰:")
        for risk in risk_analysis['high_risk']:
            print(f"  â€¢ {risk['students'][0]} â†” {risk['students'][1]}")
            print(f"    æœ€é«˜ç›¸ä¼¼åº¦: {risk['max_similarity']:.3f}")
            print(f"    å¹³å‡ç›¸ä¼¼åº¦: {risk['avg_similarity']:.3f}")
            print(f"    æª¢æ¸¬æ–¹æ³•æ•¸: {risk['detection_methods']}")
            print(f"    è­‰æ“šå¼·åº¦: {risk['evidence_strength']}")
    
    if risk_analysis['medium_risk']:
        print(f"\nâš ï¸ ä¸­ç­‰é¢¨éšªé…å°ï¼ˆéœ€è¦é—œæ³¨ï¼‰:")
        for risk in risk_analysis['medium_risk']:
            print(f"  â€¢ {risk['students'][0]} â†” {risk['students'][1]}")
            print(f"    æœ€é«˜ç›¸ä¼¼åº¦: {risk['max_similarity']:.3f}")
    
    if risk_analysis['low_risk']:
        print(f"\nâœ… ä½é¢¨éšªå­¸ç”Ÿ:")
        for student in risk_analysis['low_risk']:
            print(f"  â€¢ {student['student']}: {student['status']}")
    
    print(f"\nğŸ”¬ æŠ€è¡“å¯ä¿¡åº¦ä¿è­‰")
    print("-" * 50)
    print("âœ“ å¤šç®—æ³•äº¤å‰é©—è­‰ï¼ˆ8ç¨®æ ¸å¿ƒç®—æ³•ï¼‰")
    print("âœ“ å·¥æ¥­æ¨™æº–æª¢æ¸¬æ–¹æ³•ï¼ˆåƒè€ƒTurnitinæŠ€è¡“ï¼‰")
    print("âœ“ æ»‘å‹•çª—å£æª¢æ¸¬ï¼ˆé‡çµ„å¼æŠ„è¥²ï¼‰")
    print("âœ“ åŒç¾©è©æ„ŸçŸ¥æª¢æ¸¬ï¼ˆæ”¹å¯«å¼æŠ„è¥²ï¼‰")
    print("âœ“ å¥æ³•çµæ§‹åˆ†æï¼ˆæ·±å±¤èªç¾©æŠ„è¥²ï¼‰")
    print("âœ“ å¯èª¿é–¾å€¼ç³»çµ±ï¼ˆå½ˆæ€§æª¢æ¸¬ï¼‰")

# æ–°å¢å°ˆæ¥­æ¸¬è©¦æ¡ˆä¾‹
def professional_plagiarism_test():
    """å°ˆæ¥­ç´šæŠ„è¥²æª¢æ¸¬æ¸¬è©¦"""
    
    test_cases = [
        {
            "name": "ç›´æ¥æŠ„è¥²æ¡ˆä¾‹",
            "texts": [
                "The Czochralski method is a crystal growth technique where a seed crystal is dipped into molten silicon and slowly pulled upward while rotating to create a large single crystal.",
                "The Czochralski process is a crystal growing technique in which a seed crystal is immersed in molten silicon and gradually pulled up while rotating to form a large single crystal.",
                "Molecular beam epitaxy (MBE) is a thin film deposition technique that allows for precise control of layer thickness and composition at the atomic level."
            ],
            "names": ["Alice", "Bob", "Charlie"]
        },
        {
            "name": "åŒç¾©è©æ”¹å¯«æŠ„è¥²",
            "texts": [
                "This technique involves the controlled growth of silicon crystals using precise temperature management.",
                "This method requires the controlled development of silicon crystalline structures through accurate thermal control.",
                "Gallium arsenide devices exhibit unique optical properties that differ significantly from silicon-based components."
            ],
            "names": ["David", "Eve", "Frank"]
        },
        {
            "name": "é‡çµ„å¼æŠ„è¥²",
            "texts": [
                "Silicon solar cells achieve high efficiency through optimal band gap engineering. The photovoltaic effect converts sunlight into electrical energy.",
                "The photovoltaic effect in silicon cells converts sunlight into electrical energy. Through optimal band gap engineering, these solar cells achieve high efficiency.",
                "Quantum dots exhibit size-dependent emission wavelengths due to quantum confinement effects in nanoscale structures."
            ],
            "names": ["Grace", "Henry", "Iris"]
        }
    ]
    
    for i, case in enumerate(test_cases, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ§ª å°ˆæ¥­æ¸¬è©¦æ¡ˆä¾‹ {i}: {case['name']}")
        print(f"{'='*80}")
        
        # é¡¯ç¤ºåŸå§‹æ–‡æœ¬
        for j, (name, text) in enumerate(zip(case['names'], case['texts']), 1):
            print(f"{j}. {name}: {text}")
        
        # åŸ·è¡Œå·¥æ¥­ç´šæª¢æ¸¬
        result = industrial_grade_plagiarism_detection(case['texts'], case['names'])

# æ¸¬è©¦å‡½æ•¸æ›´æ–°
def test_alternative_methods():
    """æ¸¬è©¦æ›¿ä»£ç›¸ä¼¼åº¦æª¢æ¸¬æ–¹æ³• - æ›´æ–°ç‰ˆæœ¬"""
    print("ğŸ”¬ éAIç›¸ä¼¼åº¦æª¢æ¸¬æ–¹æ³•æ¸¬è©¦")
    print("åƒè€ƒä¸»æµç³»çµ±æŠ€è¡“æ¨™æº– (Turnitinç­‰)")
    print("="*80)
    
    professional_plagiarism_test()
