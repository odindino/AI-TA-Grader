#!/usr/bin/env python3
# alternative_similarity_methods.py
# æä¾›ä¸ä¾è³´ API çš„ç›¸ä¼¼åº¦æª¢æ¸¬æ–¹æ³•

import re
import difflib
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def calculate_text_similarity_local(texts: list[str], names: list[str] = None, threshold=0.6):
    """
    ä½¿ç”¨æœ¬åœ°æ–¹æ³•è¨ˆç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆä¸ä¾è³´ APIï¼‰
    çµåˆå¤šç¨®ç›¸ä¼¼åº¦æŒ‡æ¨™ä¾†æª¢æ¸¬æ½›åœ¨æŠ„è¥²
    """
    if len(texts) < 2:
        return [0] * len(texts)
    
    # é è™•ç†æ–‡æœ¬
    processed_texts = []
    for text in texts:
        if text and text.strip():
            # åŸºæœ¬æ–‡æœ¬æ¸…ç†
            cleaned = re.sub(r'\s+', ' ', text.strip().lower())
            # ç§»é™¤æ¨™é»ç¬¦è™Ÿï¼ˆä¿ç•™å­—æ¯æ•¸å­—ï¼‰
            cleaned = re.sub(r'[^\w\s]', ' ', cleaned)
            processed_texts.append(cleaned)
        else:
            processed_texts.append("")
    
    results = []
    detailed_results = []
    
    for i, text_i in enumerate(processed_texts):
        if not text_i.strip():
            results.append(0)
            continue
            
        max_similarity = 0
        best_match_idx = -1
        
        for j, text_j in enumerate(processed_texts):
            if i == j or not text_j.strip():
                continue
                
            # æ–¹æ³• 1: å­—ç¬¦ç´šåˆ¥ç›¸ä¼¼åº¦ (difflib)
            char_similarity = difflib.SequenceMatcher(None, text_i, text_j).ratio()
            
            # æ–¹æ³• 2: è©å½™é‡ç–Šç›¸ä¼¼åº¦
            words_i = set(text_i.split())
            words_j = set(text_j.split())
            if words_i or words_j:
                jaccard_similarity = len(words_i & words_j) / len(words_i | words_j)
            else:
                jaccard_similarity = 0
            
            # æ–¹æ³• 3: N-gram ç›¸ä¼¼åº¦
            ngram_similarity = calculate_ngram_similarity(text_i, text_j, n=3)
            
            # ç¶œåˆç›¸ä¼¼åº¦ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰
            combined_similarity = (
                char_similarity * 0.3 + 
                jaccard_similarity * 0.4 + 
                ngram_similarity * 0.3
            )
            
            if combined_similarity > max_similarity:
                max_similarity = combined_similarity
                best_match_idx = j
        
        # è¨˜éŒ„è©³ç´°çµæœ
        if max_similarity >= threshold and best_match_idx >= 0:
            student_i = names[i] if names else f"å­¸ç”Ÿ {i+1}"
            student_j = names[best_match_idx] if names else f"å­¸ç”Ÿ {best_match_idx+1}"
            detailed_results.append({
                "student_1": student_i,
                "student_2": student_j,
                "similarity": max_similarity,
                "index_1": i,
                "index_2": best_match_idx
            })
        
        # åˆ†é¡ç›¸ä¼¼åº¦ç­‰ç´šï¼ˆèª¿æ•´é–¾å€¼ä½¿å…¶æ›´æ•æ„Ÿï¼‰
        if max_similarity >= 0.75:
            results.append(2)  # é«˜åº¦ç›¸ä¼¼
        elif max_similarity >= threshold:
            results.append(1)  # ä¸­ç­‰ç›¸ä¼¼
        else:
            results.append(0)  # ç„¡æ˜é¡¯ç›¸ä¼¼
    
    return results, detailed_results

def calculate_ngram_similarity(text1: str, text2: str, n: int = 3):
    """è¨ˆç®— N-gram ç›¸ä¼¼åº¦"""
    def get_ngrams(text, n):
        words = text.split()
        return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]
    
    ngrams1 = set(get_ngrams(text1, n))
    ngrams2 = set(get_ngrams(text2, n))
    
    if not ngrams1 and not ngrams2:
        return 1.0
    if not ngrams1 or not ngrams2:
        return 0.0
    
    intersection = len(ngrams1 & ngrams2)
    union = len(ngrams1 | ngrams2)
    
    return intersection / union if union > 0 else 0

def calculate_tfidf_similarity(texts: list[str], threshold=0.6):
    """ä½¿ç”¨ TF-IDF å‘é‡åŒ–è¨ˆç®—ç›¸ä¼¼åº¦"""
    if len(texts) < 2:
        return [0] * len(texts), []
    
    # éæ¿¾ç©ºæ–‡æœ¬
    valid_texts = [text if text and text.strip() else " " for text in texts]
    
    try:
        # TF-IDF å‘é‡åŒ–
        vectorizer = TfidfVectorizer(
            stop_words='english',
            max_features=1000,
            ngram_range=(1, 2)
        )
        tfidf_matrix = vectorizer.fit_transform(valid_texts)
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        similarity_matrix = cosine_similarity(tfidf_matrix)
        np.fill_diagonal(similarity_matrix, 0)
        
        # åˆ†æçµæœ
        results = []
        detailed_results = []
        
        for i in range(len(texts)):
            max_sim = np.max(similarity_matrix[i])
            max_idx = np.argmax(similarity_matrix[i])
            
            if max_sim >= threshold:
                detailed_results.append({
                    "student_1": f"å­¸ç”Ÿ {i+1}",
                    "student_2": f"å­¸ç”Ÿ {max_idx+1}",
                    "similarity": float(max_sim),
                    "index_1": i,
                    "index_2": max_idx
                })
                 # åˆ†é¡ï¼ˆèª¿æ•´é–¾å€¼ä½¿å…¶æ›´æ•æ„Ÿï¼‰
        if max_sim >= 0.80:  # TF-IDF é–¾å€¼é™ä½
            results.append(2)
        elif max_sim >= threshold:
            results.append(1)
        else:
            results.append(0)
        
        return results, detailed_results
        
    except Exception as e:
        print(f"TF-IDF è¨ˆç®—éŒ¯èª¤: {e}")
        return [0] * len(texts), []

def comprehensive_plagiarism_check(texts: list[str], names: list[str] = None):
    """
    ç¶œåˆæŠ„è¥²æª¢æ¸¬ï¼šçµåˆå¤šç¨®æ–¹æ³•
    """
    print("ğŸ” ç¶œåˆæŠ„è¥²æª¢æ¸¬åˆ†æ")
    print("=" * 50)
    
    if len(texts) < 2:
        print("âŒ æ–‡æœ¬æ•¸é‡ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œæ¯”è¼ƒ")
        return
    
    # æ–¹æ³• 1: æœ¬åœ°æ–‡æœ¬ç›¸ä¼¼åº¦
    print("\nğŸ“ æ–¹æ³• 1: æœ¬åœ°æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ")
    local_results, local_details = calculate_text_similarity_local(texts, names, threshold=0.7)
    
    print(f"æª¢æ¸¬çµæœ:")
    for i, (name, result) in enumerate(zip(names if names else [f"å­¸ç”Ÿ {i+1}" for i in range(len(texts))], local_results)):
        status = ["âœ… ç„¡ç›¸ä¼¼", "âš ï¸ ä¸­ç­‰ç›¸ä¼¼", "ğŸš¨ é«˜åº¦ç›¸ä¼¼"][result] if result >= 0 else "âŒ éŒ¯èª¤"
        print(f"  {name}: {status}")
    
    if local_details:
        print(f"é«˜ç›¸ä¼¼åº¦é…å°:")
        for detail in local_details:
            print(f"  - {detail['student_1']} â†” {detail['student_2']}: {detail['similarity']:.3f}")
    
    # æ–¹æ³• 2: TF-IDF ç›¸ä¼¼åº¦
    print(f"\nğŸ“Š æ–¹æ³• 2: TF-IDF å‘é‡ç›¸ä¼¼åº¦åˆ†æ")
    tfidf_results, tfidf_details = calculate_tfidf_similarity(texts, threshold=0.7)
    
    print(f"æª¢æ¸¬çµæœ:")
    for i, (name, result) in enumerate(zip(names if names else [f"å­¸ç”Ÿ {i+1}" for i in range(len(texts))], tfidf_results)):
        status = ["âœ… ç„¡ç›¸ä¼¼", "âš ï¸ ä¸­ç­‰ç›¸ä¼¼", "ğŸš¨ é«˜åº¦ç›¸ä¼¼"][result] if result >= 0 else "âŒ éŒ¯èª¤"
        print(f"  {name}: {status}")
    
    if tfidf_details:
        print(f"é«˜ç›¸ä¼¼åº¦é…å°:")
        for detail in tfidf_details:
            print(f"  - {detail['student_1']} â†” {detail['student_2']}: {detail['similarity']:.3f}")
    
    # ç¶œåˆåˆ†æ
    print(f"\nğŸ¯ ç¶œåˆåˆ†æ:")
    high_risk_students = set()
    
    for i, (local, tfidf) in enumerate(zip(local_results, tfidf_results)):
        if local >= 2 or tfidf >= 2:  # ä»»ä¸€æ–¹æ³•æª¢æ¸¬åˆ°é«˜ç›¸ä¼¼åº¦
            student_name = names[i] if names else f"å­¸ç”Ÿ {i+1}"
            high_risk_students.add(student_name)
    
    if high_risk_students:
        print(f"ğŸš¨ é«˜é¢¨éšªå­¸ç”Ÿï¼ˆå¯èƒ½æ¶‰åŠæŠ„è¥²ï¼‰:")
        for student in high_risk_students:
            print(f"  - {student}")
    else:
        print(f"âœ… æœªæª¢æ¸¬åˆ°æ˜é¡¯çš„æŠ„è¥²è¡Œç‚º")
    
    return {
        "local_results": local_results,
        "tfidf_results": tfidf_results,
        "high_risk_students": list(high_risk_students),
        "local_details": local_details,
        "tfidf_details": tfidf_details
    }

def calculate_advanced_similarity(text1: str, text2: str):
    """
    è¨ˆç®—é«˜ç´šç›¸ä¼¼åº¦ï¼Œçµåˆå¤šç¨®å·¥æ¥­ç´šç®—æ³•
    åƒè€ƒTurnitinç­‰å•†æ¥­ç³»çµ±çš„æª¢æ¸¬æ–¹æ³•
    """
    if not text1.strip() or not text2.strip():
        return 0.0
    
    # é è™•ç†
    def preprocess(text):
        # è½‰å°å¯«ä¸¦è¦ç¯„åŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text.lower().strip())
        # ç§»é™¤æ¨™é»ä½†ä¿ç•™æ–‡æœ¬çµæ§‹
        text = re.sub(r'[^\w\s]', ' ', text)
        return text
    
    proc_text1 = preprocess(text1)
    proc_text2 = preprocess(text2)
    
    # 1. æœ€é•·å…¬å…±å­åºåˆ—ç›¸ä¼¼åº¦ (LCS)
    def lcs_similarity(s1, s2):
        words1 = s1.split()
        words2 = s2.split()
        
        # å‹•æ…‹è¦åŠƒè¨ˆç®—LCS
        m, n = len(words1), len(words2)
        if m == 0 or n == 0:
            return 0.0
        
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if words1[i-1] == words2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        lcs_length = dp[m][n]
        return 2.0 * lcs_length / (m + n)
    
    # 2. ç·¨è¼¯è·é›¢ç›¸ä¼¼åº¦ (Levenshtein Distance)
    def edit_distance_similarity(s1, s2):
        words1 = s1.split()
        words2 = s2.split()
        
        if len(words1) == 0 and len(words2) == 0:
            return 1.0
        if len(words1) == 0 or len(words2) == 0:
            return 0.0
        
        # è¨ˆç®—è©ç´šåˆ¥çš„ç·¨è¼¯è·é›¢
        m, n = len(words1), len(words2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(m + 1):
            dp[i][0] = i
        for j in range(n + 1):
            dp[0][j] = j
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if words1[i-1] == words2[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])
        
        max_len = max(m, n)
        return 1.0 - (dp[m][n] / max_len)
    
    # 3. èªç¾©å¡Šç›¸ä¼¼åº¦ (åŸºæ–¼é€£çºŒè©çµ„)
    def semantic_block_similarity(s1, s2, block_size=4):
        words1 = s1.split()
        words2 = s2.split()
        
        if len(words1) < block_size or len(words2) < block_size:
            return 0.0
        
        blocks1 = [' '.join(words1[i:i+block_size]) for i in range(len(words1)-block_size+1)]
        blocks2 = [' '.join(words2[i:i+block_size]) for i in range(len(words2)-block_size+1)]
        
        if not blocks1 or not blocks2:
            return 0.0
        
        common_blocks = len(set(blocks1) & set(blocks2))
        total_blocks = len(set(blocks1) | set(blocks2))
        
        return common_blocks / total_blocks if total_blocks > 0 else 0.0
    
    # 4. å­—ç¬¦ç´šåˆ¥ç›¸ä¼¼åº¦ï¼ˆdifflibï¼‰
    char_sim = difflib.SequenceMatcher(None, proc_text1, proc_text2).ratio()
    
    # 5. è©å½™é‡ç–Šç›¸ä¼¼åº¦ï¼ˆJaccardï¼‰
    words1 = set(proc_text1.split())
    words2 = set(proc_text2.split())
    jaccard_sim = len(words1 & words2) / len(words1 | words2) if (words1 | words2) else 0.0
    
    # 6. N-gram ç›¸ä¼¼åº¦
    ngram_sim = calculate_ngram_similarity(proc_text1, proc_text2, n=3)
    
    # è¨ˆç®—å„ç¨®é«˜ç´šç›¸ä¼¼åº¦
    lcs_sim = lcs_similarity(proc_text1, proc_text2)
    edit_sim = edit_distance_similarity(proc_text1, proc_text2)
    semantic_sim = semantic_block_similarity(proc_text1, proc_text2)
    
    # åŠ æ¬Šçµ„åˆæ‰€æœ‰ç›¸ä¼¼åº¦æŒ‡æ¨™
    # æ¬Šé‡åŸºæ–¼ä¸åŒç®—æ³•çš„æª¢æ¸¬èƒ½åŠ›å’Œæº–ç¢ºæ€§èª¿æ•´
    combined_similarity = (
        char_sim * 0.15 +           # å­—ç¬¦ç´šæª¢æ¸¬
        jaccard_sim * 0.20 +        # è©å½™é‡ç–Š
        ngram_sim * 0.15 +          # N-gramæ¨¡å¼
        lcs_sim * 0.20 +            # æœ€é•·å…¬å…±å­åºåˆ—
        edit_sim * 0.15 +           # ç·¨è¼¯è·é›¢
        semantic_sim * 0.15         # èªç¾©å¡Šæª¢æ¸¬
    )
    
    return combined_similarity

def calculate_text_similarity_enhanced(texts: list[str], names: list[str] = None, threshold=0.6):
    """
    å¢å¼·ç‰ˆæ–‡æœ¬ç›¸ä¼¼åº¦æª¢æ¸¬ï¼Œæ¡ç”¨å¤šç¨®å·¥æ¥­ç´šç®—æ³•
    """
    if len(texts) < 2:
        return [0] * len(texts), []
    
    results = []
    detailed_results = []
    
    for i, text_i in enumerate(texts):
        if not text_i or not text_i.strip():
            results.append(0)
            continue
            
        max_similarity = 0
        best_match_idx = -1
        
        for j, text_j in enumerate(texts):
            if i == j or not text_j or not text_j.strip():
                continue
                
            # ä½¿ç”¨å¢å¼·çš„ç›¸ä¼¼åº¦è¨ˆç®—
            similarity = calculate_advanced_similarity(text_i, text_j)
            
            if similarity > max_similarity:
                max_similarity = similarity
                best_match_idx = j
        
        # è¨˜éŒ„è©³ç´°çµæœ
        if max_similarity >= threshold and best_match_idx >= 0:
            student_i = names[i] if names else f"å­¸ç”Ÿ {i+1}"
            student_j = names[best_match_idx] if names else f"å­¸ç”Ÿ {best_match_idx+1}"
            detailed_results.append({
                "student_1": student_i,
                "student_2": student_j,
                "similarity": float(max_similarity),
                "index_1": i,
                "index_2": best_match_idx,
                "method": "enhanced_multi_algorithm"
            })
        
        # åˆ†ç´šåˆ¤å®šï¼ˆæ›´åš´æ ¼çš„é–¾å€¼ï¼‰
        if max_similarity >= 0.80:      # æé«˜é«˜ç›¸ä¼¼åº¦é–¾å€¼
            results.append(2)           # é«˜åº¦ç›¸ä¼¼
        elif max_similarity >= threshold:
            results.append(1)           # ä¸­ç­‰ç›¸ä¼¼
        else:
            results.append(0)           # ç„¡æ˜é¡¯ç›¸ä¼¼
    
    return results, detailed_results

# æ¸¬è©¦å‡½æ•¸
def test_alternative_methods():
    """æ¸¬è©¦æ›¿ä»£ç›¸ä¼¼åº¦æª¢æ¸¬æ–¹æ³•"""
    
    # æ¸¬è©¦æ¡ˆä¾‹
    test_cases = [
        {
            "name": "æ˜é¡¯æŠ„è¥²æ¡ˆä¾‹",
            "texts": [
                "The Czochralski method is a crystal growth technique where a seed crystal is dipped into molten silicon and slowly pulled upward while rotating.",
                "The Czochralski process is a crystal growing technique in which a seed crystal is immersed in molten silicon and gradually pulled up while rotating.",
                "Molecular beam epitaxy is a completely different thin film deposition technique."
            ],
            "names": ["Alice", "Bob", "Charlie"]
        },
        {
            "name": "æ­£å¸¸å·®ç•°æ¡ˆä¾‹",
            "texts": [
                "Silicon solar cells achieve efficiency around 20% due to optimal band gap.",
                "Gallium arsenide devices have different optical properties than silicon.",
                "Quantum dots exhibit size-dependent emission wavelengths."
            ],
            "names": ["David", "Eve", "Frank"]
        }
    ]
    
    for i, case in enumerate(test_cases, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ æ¸¬è©¦æ¡ˆä¾‹ {i}: {case['name']}")
        print(f"{'='*60}")
        
        for j, (name, text) in enumerate(zip(case['names'], case['texts']), 1):
            print(f"{j}. {name}: {text}")
        
        comprehensive_plagiarism_check(case['texts'], case['names'])

if __name__ == "__main__":
    test_alternative_methods()
