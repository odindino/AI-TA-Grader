#!/usr/bin/env python3
# alternative_similarity_methods.py
# æä¾›ä¸ä¾è³´ API çš„ç›¸ä¼¼åº¦æª¢æ¸¬æ–¹æ³•

import re
import difflib
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def calculate_text_similarity_local(texts: list[str], names: list[str] = None, threshold=0.6):
    """
    ä½¿ç”¨æœ¬åœ°æ–¹æ³•è¨ˆç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆä¸ä¾è³´ APIï¼‰
    çµåˆå¤šç¨®ç›¸ä¼¼åº¦æŒ‡æ¨™ä¾†æª¢æ¸¬æ½›åœ¨æŠ„è¥²
    """
    if len(texts) < 2:
        return [0] * len(texts)
    
    # é è™•ç†æ–‡æœ¬
    processed_texts = []
    for text in texts:
        if text and text.strip():
            # åŸºæœ¬æ–‡æœ¬æ¸…ç†
            cleaned = re.sub(r'\s+', ' ', text.strip().lower())
            # ç§»é™¤æ¨™é»ç¬¦è™Ÿï¼ˆä¿ç•™å­—æ¯æ•¸å­—ï¼‰
            cleaned = re.sub(r'[^\w\s]', ' ', cleaned)
            processed_texts.append(cleaned)
        else:
            processed_texts.append("")
    
    results = []
    detailed_results = []
    
    for i, text_i in enumerate(processed_texts):
        if not text_i.strip():
            results.append(0)
            continue
            
        max_similarity = 0
        best_match_idx = -1
        
        for j, text_j in enumerate(processed_texts):
            if i == j or not text_j.strip():
                continue
                
            # æ–¹æ³• 1: å­—ç¬¦ç´šåˆ¥ç›¸ä¼¼åº¦ (difflib)
            char_similarity = difflib.SequenceMatcher(None, text_i, text_j).ratio()
            
            # æ–¹æ³• 2: è©å½™é‡ç–Šç›¸ä¼¼åº¦
            words_i = set(text_i.split())
            words_j = set(text_j.split())
            if words_i or words_j:
                jaccard_similarity = len(words_i & words_j) / len(words_i | words_j)
            else:
                jaccard_similarity = 0
            
            # æ–¹æ³• 3: N-gram ç›¸ä¼¼åº¦
            ngram_similarity = calculate_ngram_similarity(text_i, text_j, n=3)
            
            # ç¶œåˆç›¸ä¼¼åº¦ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰
            combined_similarity = (
                char_similarity * 0.3 + 
                jaccard_similarity * 0.4 + 
                ngram_similarity * 0.3
            )
            
            if combined_similarity > max_similarity:
                max_similarity = combined_similarity
                best_match_idx = j
        
        # è¨˜éŒ„è©³ç´°çµæœ
        if max_similarity >= threshold and best_match_idx >= 0:
            student_i = names[i] if names else f"å­¸ç”Ÿ {i+1}"
            student_j = names[best_match_idx] if names else f"å­¸ç”Ÿ {best_match_idx+1}"
            detailed_results.append({
                "student_1": student_i,
                "student_2": student_j,
                "similarity": max_similarity,
                "index_1": i,
                "index_2": best_match_idx
            })
        
        # åˆ†é¡ç›¸ä¼¼åº¦ç­‰ç´šï¼ˆèª¿æ•´é–¾å€¼ä½¿å…¶æ›´æ•æ„Ÿï¼‰
        if max_similarity >= 0.75:
            results.append(2)  # é«˜åº¦ç›¸ä¼¼
        elif max_similarity >= threshold:
            results.append(1)  # ä¸­ç­‰ç›¸ä¼¼
        else:
            results.append(0)  # ç„¡æ˜é¡¯ç›¸ä¼¼
    
    return results, detailed_results

def calculate_ngram_similarity(text1: str, text2: str, n: int = 3):
    """è¨ˆç®— N-gram ç›¸ä¼¼åº¦"""
    def get_ngrams(text, n):
        words = text.split()
        return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]
    
    ngrams1 = set(get_ngrams(text1, n))
    ngrams2 = set(get_ngrams(text2, n))
    
    if not ngrams1 and not ngrams2:
        return 1.0
    if not ngrams1 or not ngrams2:
        return 0.0
    
    intersection = len(ngrams1 & ngrams2)
    union = len(ngrams1 | ngrams2)
    
    return intersection / union if union > 0 else 0

def calculate_tfidf_similarity(texts: list[str], threshold=0.6):
    """ä½¿ç”¨ TF-IDF å‘é‡åŒ–è¨ˆç®—ç›¸ä¼¼åº¦"""
    if len(texts) < 2:
        return [0] * len(texts), []
    
    # éæ¿¾ç©ºæ–‡æœ¬
    valid_texts = [text if text and text.strip() else " " for text in texts]
    
    try:
        # TF-IDF å‘é‡åŒ–
        vectorizer = TfidfVectorizer(
            stop_words='english',
            max_features=1000,
            ngram_range=(1, 2)
        )
        tfidf_matrix = vectorizer.fit_transform(valid_texts)
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        similarity_matrix = cosine_similarity(tfidf_matrix)
        np.fill_diagonal(similarity_matrix, 0)
        
        # åˆ†æçµæœ
        results = []
        detailed_results = []
        
        for i in range(len(texts)):
            max_sim = np.max(similarity_matrix[i])
            max_idx = np.argmax(similarity_matrix[i])
            
            if max_sim >= threshold:
                detailed_results.append({
                    "student_1": f"å­¸ç”Ÿ {i+1}",
                    "student_2": f"å­¸ç”Ÿ {max_idx+1}",
                    "similarity": float(max_sim),
                    "index_1": i,
                    "index_2": max_idx
                })
                 # åˆ†é¡ï¼ˆèª¿æ•´é–¾å€¼ä½¿å…¶æ›´æ•æ„Ÿï¼‰
        if max_sim >= 0.80:  # TF-IDF é–¾å€¼é™ä½
            results.append(2)
        elif max_sim >= threshold:
            results.append(1)
        else:
            results.append(0)
        
        return results, detailed_results
        
    except Exception as e:
        print(f"TF-IDF è¨ˆç®—éŒ¯èª¤: {e}")
        return [0] * len(texts), []

def comprehensive_plagiarism_check(texts: list[str], names: list[str] = None):
    """
    ç¶œåˆæŠ„è¥²æª¢æ¸¬ï¼šçµåˆå¤šç¨®æ–¹æ³•
    """
    print("ğŸ” ç¶œåˆæŠ„è¥²æª¢æ¸¬åˆ†æ")
    print("=" * 50)
    
    if len(texts) < 2:
        print("âŒ æ–‡æœ¬æ•¸é‡ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œæ¯”è¼ƒ")
        return
    
    # æ–¹æ³• 1: æœ¬åœ°æ–‡æœ¬ç›¸ä¼¼åº¦
    print("\nğŸ“ æ–¹æ³• 1: æœ¬åœ°æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ")
    local_results, local_details = calculate_text_similarity_local(texts, names, threshold=0.7)
    
    print(f"æª¢æ¸¬çµæœ:")
    for i, (name, result) in enumerate(zip(names if names else [f"å­¸ç”Ÿ {i+1}" for i in range(len(texts))], local_results)):
        status = ["âœ… ç„¡ç›¸ä¼¼", "âš ï¸ ä¸­ç­‰ç›¸ä¼¼", "ğŸš¨ é«˜åº¦ç›¸ä¼¼"][result] if result >= 0 else "âŒ éŒ¯èª¤"
        print(f"  {name}: {status}")
    
    if local_details:
        print(f"é«˜ç›¸ä¼¼åº¦é…å°:")
        for detail in local_details:
            print(f"  - {detail['student_1']} â†” {detail['student_2']}: {detail['similarity']:.3f}")
    
    # æ–¹æ³• 2: TF-IDF ç›¸ä¼¼åº¦
    print(f"\nğŸ“Š æ–¹æ³• 2: TF-IDF å‘é‡ç›¸ä¼¼åº¦åˆ†æ")
    tfidf_results, tfidf_details = calculate_tfidf_similarity(texts, threshold=0.7)
    
    print(f"æª¢æ¸¬çµæœ:")
    for i, (name, result) in enumerate(zip(names if names else [f"å­¸ç”Ÿ {i+1}" for i in range(len(texts))], tfidf_results)):
        status = ["âœ… ç„¡ç›¸ä¼¼", "âš ï¸ ä¸­ç­‰ç›¸ä¼¼", "ğŸš¨ é«˜åº¦ç›¸ä¼¼"][result] if result >= 0 else "âŒ éŒ¯èª¤"
        print(f"  {name}: {status}")
    
    if tfidf_details:
        print(f"é«˜ç›¸ä¼¼åº¦é…å°:")
        for detail in tfidf_details:
            print(f"  - {detail['student_1']} â†” {detail['student_2']}: {detail['similarity']:.3f}")
    
    # ç¶œåˆåˆ†æ
    print(f"\nğŸ¯ ç¶œåˆåˆ†æ:")
    high_risk_students = set()
    
    for i, (local, tfidf) in enumerate(zip(local_results, tfidf_results)):
        if local >= 2 or tfidf >= 2:  # ä»»ä¸€æ–¹æ³•æª¢æ¸¬åˆ°é«˜ç›¸ä¼¼åº¦
            student_name = names[i] if names else f"å­¸ç”Ÿ {i+1}"
            high_risk_students.add(student_name)
    
    if high_risk_students:
        print(f"ğŸš¨ é«˜é¢¨éšªå­¸ç”Ÿï¼ˆå¯èƒ½æ¶‰åŠæŠ„è¥²ï¼‰:")
        for student in high_risk_students:
            print(f"  - {student}")
    else:
        print(f"âœ… æœªæª¢æ¸¬åˆ°æ˜é¡¯çš„æŠ„è¥²è¡Œç‚º")
    
    return {
        "local_results": local_results,
        "tfidf_results": tfidf_results,
        "high_risk_students": list(high_risk_students),
        "local_details": local_details,
        "tfidf_details": tfidf_details
    }

# æ¸¬è©¦å‡½æ•¸
def test_alternative_methods():
    """æ¸¬è©¦æ›¿ä»£ç›¸ä¼¼åº¦æª¢æ¸¬æ–¹æ³•"""
    
    # æ¸¬è©¦æ¡ˆä¾‹
    test_cases = [
        {
            "name": "æ˜é¡¯æŠ„è¥²æ¡ˆä¾‹",
            "texts": [
                "The Czochralski method is a crystal growth technique where a seed crystal is dipped into molten silicon and slowly pulled upward while rotating.",
                "The Czochralski process is a crystal growing technique in which a seed crystal is immersed in molten silicon and gradually pulled up while rotating.",
                "Molecular beam epitaxy is a completely different thin film deposition technique."
            ],
            "names": ["Alice", "Bob", "Charlie"]
        },
        {
            "name": "æ­£å¸¸å·®ç•°æ¡ˆä¾‹",
            "texts": [
                "Silicon solar cells achieve efficiency around 20% due to optimal band gap.",
                "Gallium arsenide devices have different optical properties than silicon.",
                "Quantum dots exhibit size-dependent emission wavelengths."
            ],
            "names": ["David", "Eve", "Frank"]
        }
    ]
    
    for i, case in enumerate(test_cases, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ æ¸¬è©¦æ¡ˆä¾‹ {i}: {case['name']}")
        print(f"{'='*60}")
        
        for j, (name, text) in enumerate(zip(case['names'], case['texts']), 1):
            print(f"{j}. {name}: {text}")
        
        comprehensive_plagiarism_check(case['texts'], case['names'])

if __name__ == "__main__":
    test_alternative_methods()
