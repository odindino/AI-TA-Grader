#!/usr/bin/env python3
# improved_similarity_recommendations.py
# ç›¸ä¼¼åº¦æª¢æ¸¬æ”¹é€²å»ºè­°å’Œå¯¦ç”¨å·¥å…·

"""
é‡å° AI-TA-Grader çš„ç›¸ä¼¼åº¦æª¢æ¸¬æ”¹é€²å»ºè­°

=== ç•¶å‰å¯¦ç¾è©•ä¼° ===
âœ… å„ªç§€çš„åŸºç¤ï¼š
- Google text-embedding-004 æ¨¡å‹ï¼ˆèªæ„ç†è§£èƒ½åŠ›å¼·ï¼‰
- åˆç†çš„é–¾å€¼è¨­å®šï¼ˆ0.85 é«˜é¢¨éšªï¼Œ0.70 ä¸­é¢¨éšªï¼‰
- æ–‡æœ¬é•·åº¦ç¯©é¸ï¼ˆ50 å­—ç¬¦æœ€å°å€¼ï¼‰
- è©³ç´°çš„é…å°ä¿¡æ¯å’Œæ—¥èªŒè¨˜éŒ„
- é©ç•¶çš„éŒ¯èª¤è™•ç†

=== æ¨è–¦çš„æ”¹é€²ç­–ç•¥ ===

1. ã€ä¿æŒç¾æœ‰æ–¹æ³•ç‚ºä¸»ã€‘- ç¹¼çºŒä½¿ç”¨ Google Embeddings
2. ã€æ·»åŠ é æª¢æ¸¬å±¤ã€‘- å¿«é€Ÿéæ¿¾æ˜é¡¯æ¡ˆä¾‹
3. ã€å¢å¼·å ±å‘ŠåŠŸèƒ½ã€‘- æä¾›æ›´è©³ç´°çš„åˆ†æ
4. ã€å‚™ç”¨æ–¹æ³•ã€‘- ç•¶ API ä¸å¯ç”¨æ™‚çš„æ›¿ä»£æ–¹æ¡ˆ

=== å…·é«”æ”¹é€²å»ºè­° ===
"""

import re
import logging
from typing import List, Dict, Tuple
import numpy as np
import google.generativeai as genai
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class ImprovedSimilarityDetector:
    """
    æ”¹é€²ç‰ˆç›¸ä¼¼åº¦æª¢æ¸¬å™¨ - åŸºæ–¼ç¾æœ‰ä»£ç¢¼çš„å¢å¼·ç‰ˆæœ¬
    """
    
    def __init__(self):
        self.min_length = 50
        self.high_threshold = 0.85
        self.mid_threshold = 0.70
    
    def quick_precheck(self, texts: List[str]) -> Dict:
        """
        å¿«é€Ÿé æª¢ï¼šè­˜åˆ¥æ˜é¡¯çš„ç›¸åŒæˆ–è¿‘ä¼¼ç›¸åŒç­”æ¡ˆ
        åœ¨é€²è¡Œ API èª¿ç”¨å‰å…ˆéæ¿¾æ˜é¡¯æ¡ˆä¾‹
        """
        precheck_results = {
            "identical_pairs": [],
            "very_similar_pairs": [],
            "short_texts": [],
            "empty_texts": []
        }
        
        processed = []
        for i, text in enumerate(texts):
            if not text or not text.strip():
                precheck_results["empty_texts"].append(i)
                processed.append("")
            elif len(text.strip()) < self.min_length:
                precheck_results["short_texts"].append(i)
                processed.append(text.strip().lower())
            else:
                # æ¨™æº–åŒ–è™•ç†
                cleaned = re.sub(r'\s+', ' ', text.strip().lower())
                processed.append(cleaned)
        
        # æª¢æŸ¥å®Œå…¨ç›¸åŒ
        for i in range(len(processed)):
            for j in range(i + 1, len(processed)):
                if processed[i] and processed[j]:
                    if processed[i] == processed[j]:
                        precheck_results["identical_pairs"].append((i, j))
                    elif self._simple_similarity(processed[i], processed[j]) > 0.95:
                        precheck_results["very_similar_pairs"].append((i, j))
        
        return precheck_results
    
    def _simple_similarity(self, text1: str, text2: str) -> float:
        """ç°¡å–®çš„æ–‡æœ¬ç›¸ä¼¼åº¦è¨ˆç®—ï¼ˆè©å½™é‡ç–Šï¼‰"""
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 and not words2:
            return 1.0
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def enhanced_embedding_similarity(self, texts: List[str], names: List[str] = None) -> Dict:
        """
        å¢å¼·ç‰ˆ Embedding ç›¸ä¼¼åº¦æª¢æ¸¬
        åŸºæ–¼ç¾æœ‰ä»£ç¢¼ï¼Œå¢åŠ æ›´è©³ç´°çš„åˆ†æå’Œå ±å‘Š
        """
        # é æª¢æ¸¬
        precheck = self.quick_precheck(texts)
        
        # æº–å‚™çµæœçµæ§‹
        results = {
            "method": "enhanced_embedding",
            "precheck": precheck,
            "similarity_flags": [],
            "detailed_pairs": [],
            "similarity_matrix": None,
            "statistics": {}
        }
        
        try:
            # æ–‡æœ¬é è™•ç†ï¼ˆèˆ‡åŸä»£ç¢¼ä¸€è‡´ï¼‰
            processed_texts = []
            for text in texts:
                if text and text.strip():
                    cleaned = re.sub(r'\s+', ' ', text.strip())
                    processed_texts.append(cleaned)
                else:
                    processed_texts.append(" ")
            
            # æª¢æŸ¥æœ‰æ•ˆæ–‡æœ¬æ•¸é‡
            valid_texts = [t for t in processed_texts if len(t.strip()) >= self.min_length]
            if len(valid_texts) < 2:
                results["similarity_flags"] = [0] * len(texts)
                results["statistics"]["status"] = "insufficient_texts"
                return results
            
            # è¨ˆç®— Embeddingï¼ˆä½¿ç”¨èˆ‡åŸä»£ç¢¼ç›¸åŒçš„æ¨¡å‹ï¼‰
            embedding_result = genai.embed_content(
                model="models/text-embedding-004",
                content=processed_texts,
                task_type="RETRIEVAL_DOCUMENT"
            )
            embeddings = embedding_result['embedding']
            
            # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
            sim_matrix = cosine_similarity(embeddings)
            np.fill_diagonal(sim_matrix, 0)
            results["similarity_matrix"] = sim_matrix.tolist()
            
            # åˆ†ææ¯å€‹å­¸ç”Ÿ
            similarity_flags = []
            detailed_pairs = []
            
            for i in range(len(texts)):
                if len(processed_texts[i].strip()) < self.min_length:
                    similarity_flags.append(0)
                    continue
                
                max_sim = np.max(sim_matrix[i])
                max_idx = np.argmax(sim_matrix[i])
                
                # åˆ†é¡ï¼ˆä½¿ç”¨åŸä»£ç¢¼çš„é–¾å€¼ï¼‰
                if max_sim >= self.high_threshold:
                    flag = 2
                elif max_sim >= self.mid_threshold:
                    flag = 1
                else:
                    flag = 0
                
                similarity_flags.append(flag)
                
                # è¨˜éŒ„è©³ç´°ä¿¡æ¯
                if max_sim >= self.mid_threshold:
                    student_i = names[i] if names else f"å­¸ç”Ÿ {i+1}"
                    student_j = names[max_idx] if names else f"å­¸ç”Ÿ {max_idx+1}"
                    
                    detailed_pairs.append({
                        "student_1": student_i,
                        "student_2": student_j,
                        "similarity": float(max_sim),
                        "risk_level": "high" if flag == 2 else "medium",
                        "index_1": i,
                        "index_2": max_idx,
                        "text_length_1": len(processed_texts[i]),
                        "text_length_2": len(processed_texts[max_idx])
                    })
            
            results["similarity_flags"] = similarity_flags
            results["detailed_pairs"] = detailed_pairs
            
            # çµ±è¨ˆä¿¡æ¯
            results["statistics"] = {
                "status": "completed",
                "total_students": len(texts),
                "valid_texts": len(valid_texts),
                "high_risk_count": similarity_flags.count(2),
                "medium_risk_count": similarity_flags.count(1),
                "safe_count": similarity_flags.count(0),
                "error_count": similarity_flags.count(-1),
                "average_similarity": float(np.mean(sim_matrix[sim_matrix > 0])) if np.any(sim_matrix > 0) else 0.0,
                "max_similarity": float(np.max(sim_matrix)),
                "identical_pairs": len(precheck["identical_pairs"]),
                "very_similar_pairs": len(precheck["very_similar_pairs"])
            }
            
        except Exception as e:
            logging.error(f"Enhanced embedding similarity error: {e}")
            results["similarity_flags"] = [-1] * len(texts)
            results["statistics"]["status"] = "error"
            results["statistics"]["error"] = str(e)
        
        return results
    
    def generate_detailed_report(self, results: Dict, names: List[str] = None) -> str:
        """
        ç”Ÿæˆè©³ç´°çš„æª¢æ¸¬å ±å‘Š
        """
        report = ["=" * 60]
        report.append("ğŸ“‹ ç›¸ä¼¼åº¦æª¢æ¸¬è©³ç´°å ±å‘Š")
        report.append("=" * 60)
        
        stats = results["statistics"]
        
        # ç¸½é«”çµ±è¨ˆ
        report.append(f"\nğŸ“Š æª¢æ¸¬çµ±è¨ˆ:")
        report.append(f"  â€¢ ç¸½å­¸ç”Ÿæ•¸: {stats.get('total_students', 0)}")
        report.append(f"  â€¢ æœ‰æ•ˆæ–‡æœ¬: {stats.get('valid_texts', 0)}")
        report.append(f"  â€¢ æª¢æ¸¬ç‹€æ…‹: {stats.get('status', 'unknown')}")
        
        if stats.get('status') == 'completed':
            report.append(f"  â€¢ é«˜é¢¨éšª: {stats.get('high_risk_count', 0)} äºº")
            report.append(f"  â€¢ ä¸­é¢¨éšª: {stats.get('medium_risk_count', 0)} äºº")
            report.append(f"  â€¢ å®‰å…¨: {stats.get('safe_count', 0)} äºº")
            report.append(f"  â€¢ å¹³å‡ç›¸ä¼¼åº¦: {stats.get('average_similarity', 0):.3f}")
            report.append(f"  â€¢ æœ€é«˜ç›¸ä¼¼åº¦: {stats.get('max_similarity', 0):.3f}")
        
        # é æª¢æ¸¬çµæœ
        precheck = results.get("precheck", {})
        if precheck.get("identical_pairs"):
            report.append(f"\nğŸ”´ å®Œå…¨ç›¸åŒçš„ç­”æ¡ˆ:")
            for i, j in precheck["identical_pairs"]:
                name_i = names[i] if names else f"å­¸ç”Ÿ {i+1}"
                name_j = names[j] if names else f"å­¸ç”Ÿ {j+1}"
                report.append(f"  â€¢ {name_i} â†” {name_j}")
        
        # è©³ç´°é…å°
        detailed_pairs = results.get("detailed_pairs", [])
        if detailed_pairs:
            report.append(f"\nğŸš¨ ç›¸ä¼¼åº¦é…å°è©³æƒ…:")
            sorted_pairs = sorted(detailed_pairs, key=lambda x: x["similarity"], reverse=True)
            
            for pair in sorted_pairs:
                emoji = "ğŸ”´" if pair["risk_level"] == "high" else "ğŸŸ¡"
                report.append(f"  {emoji} {pair['student_1']} â†” {pair['student_2']}")
                report.append(f"     ç›¸ä¼¼åº¦: {pair['similarity']:.3f} ({pair['risk_level']})")
                report.append(f"     æ–‡æœ¬é•·åº¦: {pair['text_length_1']} vs {pair['text_length_2']} å­—ç¬¦")
        
        # å»ºè­°
        report.append(f"\nğŸ’¡ æª¢æ¸¬å»ºè­°:")
        if stats.get('high_risk_count', 0) > 0:
            report.append("  â€¢ ğŸš¨ ç™¼ç¾é«˜é¢¨éšªç›¸ä¼¼ç­”æ¡ˆï¼Œå»ºè­°äººå·¥è©³ç´°å¯©æŸ¥")
        if stats.get('medium_risk_count', 0) > 0:
            report.append("  â€¢ âš ï¸ ç™¼ç¾ä¸­é¢¨éšªç›¸ä¼¼ç­”æ¡ˆï¼Œå»ºè­°é‡é»é—œæ³¨")
        if stats.get('identical_pairs', 0) > 0:
            report.append("  â€¢ ğŸ”´ ç™¼ç¾å®Œå…¨ç›¸åŒç­”æ¡ˆï¼Œå¼·çƒˆå»ºè­°èª¿æŸ¥")
        if stats.get('status') == 'completed' and stats.get('high_risk_count', 0) == 0:
            report.append("  â€¢ âœ… æœªç™¼ç¾æ˜é¡¯æŠ„è¥²è¡Œç‚º")
        
        return "\n".join(report)

# === ä½¿ç”¨å»ºè­° ===

def implementation_recommendations():
    """
    å¯¦æ–½å»ºè­°ï¼šå¦‚ä½•åœ¨ç¾æœ‰ç³»çµ±ä¸­æ‡‰ç”¨é€™äº›æ”¹é€²
    """
    print("""
ğŸ¯ å¯¦æ–½å»ºè­°ï¼š

1. ã€çŸ­æœŸæ”¹é€²ã€‘- å¢å¼·ç¾æœ‰å‡½æ•¸
   â€¢ åœ¨ calculate_similarity_flags() ä¸­æ·»åŠ é æª¢æ¸¬
   â€¢ å¢åŠ æ›´è©³ç´°çš„æ—¥èªŒè¼¸å‡º
   â€¢ æä¾›é…å°è©³æƒ…çš„ JSON è¼¸å‡º

2. ã€ä¸­æœŸæ”¹é€²ã€‘- æ·»åŠ å‚™ç”¨æ–¹æ³•
   â€¢ ç•¶ API å¤±æ•—æ™‚ï¼Œè‡ªå‹•åˆ‡æ›åˆ° TF-IDF æ–¹æ³•
   â€¢ æ·»åŠ å®Œå…¨ç›¸åŒæª¢æ¸¬ä½œç‚ºç¬¬ä¸€é“é˜²ç·š
   â€¢ æä¾›é›¢ç·šæ¨¡å¼

3. ã€é•·æœŸæ”¹é€²ã€‘- æ™ºèƒ½åŒ–æª¢æ¸¬
   â€¢ æ ¹æ“šé¡Œç›®é¡å‹èª¿æ•´é–¾å€¼
   â€¢ å­¸ç¿’å¸¸è¦‹çš„æ­£ç¢ºç­”æ¡ˆæ¨¡å¼
   â€¢ æä¾›ç›¸ä¼¼åº¦åˆ†å¸ƒåˆ†æ

4. ã€å ±å‘Šæ”¹é€²ã€‘- æ›´å¥½çš„ç”¨æˆ¶é«”é©—
   â€¢ åœ¨å‰ç«¯é¡¯ç¤ºç›¸ä¼¼åº¦é…å°
   â€¢ æä¾›å¯ä¸‹è¼‰çš„æª¢æ¸¬å ±å‘Š
   â€¢ æ·»åŠ ç›¸ä¼¼æ–‡æœ¬çš„ä¸¦æ’æ¯”è¼ƒè¦–åœ–

=== ä»£ç¢¼æ•´åˆç¤ºä¾‹ ===

# åœ¨ analyzer.py ä¸­çš„ç°¡å–®æ”¹é€²ï¼š

def enhanced_calculate_similarity_flags(texts, names=None, hi=0.85, mid=0.70, min_length=50):
    \"\"\"æ”¹é€²ç‰ˆç›¸ä¼¼åº¦æª¢æ¸¬ - åŸºæ–¼ç¾æœ‰ä»£ç¢¼\"\"\"
    
    # 1. é æª¢æ¸¬éšæ®µ
    identical_pairs = []
    for i in range(len(texts)):
        for j in range(i + 1, len(texts)):
            if texts[i] and texts[j] and texts[i].strip().lower() == texts[j].strip().lower():
                identical_pairs.append((i, j))
                logging.warning(f"ç™¼ç¾å®Œå…¨ç›¸åŒç­”æ¡ˆ: {names[i] if names else f'å­¸ç”Ÿ{i+1}'} â†” {names[j] if names else f'å­¸ç”Ÿ{j+1}'}")
    
    # 2. åŸæœ‰çš„ embedding æª¢æ¸¬
    # ... ç¾æœ‰ä»£ç¢¼ ...
    
    # 3. å¢å¼·å ±å‘Š
    if high_similarity_pairs:
        logging.info(f"ç›¸ä¼¼åº¦æª¢æ¸¬æ‘˜è¦:")
        logging.info(f"  â€¢ å®Œå…¨ç›¸åŒ: {len(identical_pairs)} å°")
        logging.info(f"  â€¢ é«˜ç›¸ä¼¼åº¦: {len([p for p in high_similarity_pairs if 'ç›¸ä¼¼åº¦: 0.8' in p])} å°")
        logging.info(f"  â€¢ å¹³å‡ç›¸ä¼¼åº¦: {np.mean([...]):.3f}")
    
    return similarity_flags

""")

if __name__ == "__main__":
    implementation_recommendations()
